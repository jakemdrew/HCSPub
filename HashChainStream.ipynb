{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d308e0a-6551-443a-af0c-772566c7f8d5",
   "metadata": {},
   "source": [
    "# Publication Plan\n",
    "\n",
    "## Publication One \n",
    "\n",
    "**Hash-Chain Augmentation: Deterministic, Self-Healing Hash Tables without Global Rehash**\n",
    "\n",
    "We introduce Hash-Chain Augmentation (HCA), a technique that replaces fixed hashes in hash-indexed data structures with a prefix-stable, extendable hash stream. HCA yields deterministic addressing, local growth without global rehash, and self-healing reconstruction from keys alone. We instantiate HCA in two high-performance designs—a Radix HCA and an Anchor HCA—and show competitive throughput with significantly flatter p95/p99 tails than CPython dict, while enabling persistence, sharding, and distributed repair.\n",
    "\n",
    "### Contribution: Hash-Chain Augmentation\n",
    "    \n",
    "Given any hash-indexed structure (open addressing, chaining, radix/HAMT, extendible hashing, cuckoo, etc.), replacing the fixed hash with a deterministic, prefix-stable HashChainStream (HCS) yields:\n",
    "* **Prefix stability & unbounded entropy:** Consume additional bits as needed without changing earlier bits; no fixed hash width.\n",
    "* **Deterministic addressing & self-healing:** The address is a pure function of (key, policy, chain version), enabling rebuild/verification from keys alone, straightforward sharding by prefix, and distributed repair.\n",
    "* **No global rehash:** Growth is local (deeper prefixes or local splits) rather than global resizes.\n",
    "* **Versioning / domain separation:** A **person** tag (or multi-seed) creates independent namespaces and controlled evolution.\n",
    "\n",
    "We formalize this Hash-Chain Augmentation (HCA) layer and show how to apply it to multiple table families.\n",
    "\n",
    "### Implementations that showcase HCA\n",
    "\n",
    "1) **Radix / nibble-trie (RAM)** - direct addressing by prefix bits\n",
    "   * One key per leaf (or tiny microtables).\n",
    "   * Zero global resize; perfect determinism; clean “storage paths as addresses”.\n",
    "2) **Anchor HCA (flat, cache-aware)** - closer to Python dict\n",
    "   * 1-hop lookups via (depth, prefix) index (or compact two-tier radix array).\n",
    "   * Local growth by bumping d on hot regions; never move existing keys.\n",
    "   * Direct comparison against contiguous-array competitors (dict, robin-hood, absl::flat_hash_map).\n",
    "\n",
    "#### Cython vs C/Rust\n",
    "\n",
    "* **C/Rust (recommended):** control layout (cache lines, packed slots), prefetch, branch-free probes. Needed for ns/op parity with dict.\n",
    "* **Cython (iteration):** OK for prototypes; ensure nogil, typed memoryviews, POD structs; avoid Python objects on the hot path.\n",
    "* **Python:** Used to quicky formalize the final algorithm\n",
    "\n",
    "**Plan:** (i) Python reference; (ii) Cython prototype; (iii) Rust/C final numbers.\n",
    "\n",
    "### Publication 1 structure\n",
    "1) **Introduction:** **Problem:** global resizes, nondeterminism, recovery pain. **Thesis:** HCA adds determinism/self-healing with O(1) behavior and competitive latency.\n",
    "2) **HashChainStream:** Definition; prefix stability; domain separation; cost model; MSB-first nibble semantics.\n",
    "3) **HCA Framework:** Abstract interface (bit stream + policy). Local split policy; one-key vs microtable trade-offs.\n",
    "4) **Implementations:**\n",
    "   * **Radix HCA (RAM):** algorithm, invariants, complexity, memory layout.\n",
    "   * **Anchor HCA (flat)** slot layout (fingerprint, key-id/value), probe loop, 1-hop index.\n",
    "5) **Self-healing & persistence:** Formal statement: reconstruct from keys; per-anchor repair; proof sketch and failure modes.\n",
    "6) **Evaluation:** \n",
    "   * **Baselines:** CPython dict, a high-quality native map (absl::flat_hash_map or robin_hood), and a HAMT/radix baseline if available.\n",
    "   * **Datasets:** uniform/Zipf/skew; adversarial long-common-prefix; varying key lengths; payload sizes.\n",
    "   * **Workloads:** 95/5 get/set, 50/50, bulk insert, delete churn, growth to 2×/4×.\n",
    "   * **Metrics:** p50/p95/p99 latency, ops/sec, probes/hops/op, LLC/TLB misses/op (perf), bytes/entry (index + metadata).\n",
    "   * **Ablations:** fingerprints on/off; microtable size; depth policy (target vs cap); HashChainStream PRF choice.\n",
    "7) **Discussion:** Where HCA wins (tails, persistence, huge maps, sharding) vs dict (small in-RAM, iteration order, constant factors).\n",
    "8) **Related Work:** dict, HAMT, extendible hashing, crit-bit/ART, CAS systems.\n",
    "9) **Conclusion & future work:** On-disk CAS, verifiable proofs, COW snapshots, concurrent algorithms.\n",
    "\n",
    "### Possible Extras\n",
    "* **Figures**\n",
    "  1) **Radix HCA insert:** collision, deeper split, two leaves.\n",
    "  2) **Anchor HCA lookup:** (d,prefix) → 1-hop microtable.\n",
    "* **Pseudocode boxes:** for Insert/Get/Delete for both implementations (≤25 lines each).\n",
    "* **Invariants/Theorem box (short):**\n",
    " * **Deterministic addressing:** For fixed (key, policy, HCS version), path is unique.\n",
    " * **Local growth:** Insert affects only nodes along the key’s path; existing addresses immutable.\n",
    " * **Self-healing:** Given keys (and optional insert depths), structure is reconstructible without global scan.\n",
    "* **Threats to validity:** hash-DoS considerations; PRF choice; GC/allocator effects; CPU prefetch variability.\n",
    "* **Artifact checklist:** versions, seeds, scripts, environment, perf counters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ff6571-5daa-47d9-8baf-a7f338f492e3",
   "metadata": {},
   "source": [
    "## Publication Two \n",
    "\n",
    "**Hash-Chain Augmented Content-Addressable Storage: Self-Healing SEC/XBRL Pipelines at Scale**\n",
    "\n",
    "We apply Hash-Chain Augmentation (HCA) to **content-addressable storage (CAS)**, building a storage substrate that provides deterministic addressing, distributed self-healing, and robust namespace resolution. By backing CAS with a prefix-stable HashChainStream (HCS), we eliminate global rehashing, enable per-file footer indices, and guarantee verifiable reconstruction of data layouts even under corruption or remote namespace failures. We demonstrate the design in the domain of SEC/XBRL processing, where traditional CAS pipelines suffer from broken taxonomies, disappearing schemas, and non-deterministic recovery paths.\n",
    "\n",
    "### Contribution: HCA for CAS\n",
    "\n",
    "* **Deterministic addressing & recovery:** Each object’s storage path is a prefix of its HCS, making it reproducible without central directories.\n",
    "* **Self-healing indices:** Immutable per-object footers distribute repair information across the corpus; recovery does not require scanning global metadata.\n",
    "* **Unlimited namespace resolution:** Unlike fixed-hash CAS systems (SHA-256, BLAKE3, etc.), HCS can extend prefixes on demand without rehashing or migration.\n",
    "* **Robustness in adversarial environments:** CAS built with HCA tolerates missing schemas, disappearing taxonomies, or partial corruption while preserving deterministic addressing.\n",
    "\n",
    "We formalize this **HCA-backed CAS** model and present both theoretical properties and a domain case study.\n",
    "\n",
    "### Comparison of Traditional SHA-256 CAS vs. HCA-Backed CAS\n",
    "\n",
    "| Dimension                  | Traditional CAS (SHA-256, Git/IPFS)             | HCA-Backed CAS (HashChainStream)                                    |\n",
    "|----------------------------|------------------------------------------------|---------------------------------------------------------------------|\n",
    "| **Address derivation**     | Fixed-width cryptographic hash (e.g., 256 bits).| Prefix-stable, extendable hash chain; path grows nibble by nibble.  |\n",
    "| **Entropy / namespace**    | Fixed; limited to hash width.                   | Unbounded; can extend prefixes as needed without rehash.            |\n",
    "| **Determinism**            | Deterministic, but collisions forced global rehash. | Deterministic prefix; no rehash ever required.                   |\n",
    "| **Growth behavior**        | Entire corpus “fixed width”; no local adaptivity. | Local growth only; deeper prefixes allocated per hot/colliding shard. |\n",
    "| **Fault tolerance**        | Global index required; loss → expensive rebuild. | Self-healing via distributed per-file footers; no monolithic scan. |\n",
    "| **Recovery path**          | Requires central directory or full rescan.      | Keys + footers sufficient; rebuild possible in parallel from shards. |\n",
    "| **Versioning**             | Requires new hash function / encoding scheme.   | Built-in `person`/multi-seed domain separation.                     |\n",
    "| **Schema / taxonomy drift**| No mechanism; broken refs remain unresolved.    | Deterministic prefix + self-healing namespace resolution.           |\n",
    "| **Performance (SEC/XBRL)** | Breaks on missing schemas; parser error spikes. | >99% of parser errors healed; throughput >16 filings/sec.           |\n",
    "| **Deployment**             | Mature tooling (Git, IPFS, etc.).               | Novel design; integrates with XBRL/SEC pipelines; CAS + data-struct synergy. |\n",
    "\n",
    "### Engineering details \n",
    "\n",
    "* **Layout:** Sharded directories driven by HCS nibbles; immutable `.nhash` files with compressed payload + footer indices.\n",
    "* **Failure modes:** Namespace corruption, partial shard loss, schema relocation; demonstrate per-anchor/per-object repair paths.\n",
    "* **Rebuild:** Keys alone suffice to reconstruct the store; footers accelerate recovery; distributed rebuild possible without scanning monolithic tables.\n",
    "* **Integration:** Drop-in replacement for existing CAS pipelines; transparent use in XBRL/SEC ingestion.\n",
    "\n",
    "### Domain results: SEC/XBRL throughput + error resolution\n",
    "\n",
    "We evaluate HCA-backed CAS on 164,000+ SEC filings (10-Q/10-K, 2019–present):\n",
    "\n",
    "* **Throughput:** >16 filings/sec sustained ingestion on commodity hardware.\n",
    "* **Error resolution:** Self-healing namespace mapping repaired 3,765 “Taxonomy not found” errors (>99% of parser failures), leaving only 40 unresolved.\n",
    "* **Determinism:** Verified that all reconstructed indices yield identical addressing to original, ensuring consistency across rebuilds.\n",
    "* **Impact:** Stable GAAP fact extraction (>99% coverage), improved model feature stability, reduced noise in downstream algorithmic trading.\n",
    "\n",
    "### Publication 2 structure\n",
    "1. **Introduction:** Problem: fragility of CAS pipelines under namespace churn, global rehash overhead, and recovery pain. Thesis: HCA yields deterministic, self-healing CAS.\n",
    "2. **Background:** CAS and XBRL/SEC ingestion challenges (taxonomy drift, schema loss, distributed corruption).\n",
    "3. **HashChainStream recap:** Key properties relevant to CAS (prefix stability, extendability, versioning).\n",
    "4. **HCA-backed CAS design:**\n",
    "   * Address derivation (prefix = storage path).\n",
    "   * File layout and footers.\n",
    "   * Self-healing rebuild mechanisms.\n",
    "5. **Engineering details:** layout, failure models, repair procedures, distributed rebuild.\n",
    "6. **Case study: SEC/XBRL pipeline:**\n",
    "   * Error statistics (taxonomy failures).\n",
    "   * Throughput benchmarks.\n",
    "   * Feature stability improvements.\n",
    "7. **Evaluation:**\n",
    "   * Correctness of rebuild under adversarial failures.\n",
    "   * Performance comparison with SHA-256 CAS baseline.\n",
    "   * Sensitivity analysis (footer size, shard width, compression).\n",
    "8. **Discussion:** Benefits beyond SEC/XBRL; applicability to large-scale storage systems, blockchain, verifiable archives.\n",
    "9. **Related Work:** SHA-based CAS (Git, IPFS), verifiable storage (Merkle trees), XBRL parsing systems.\n",
    "10. **Conclusion & future work:** Distributed CAS, integration with HCA hash tables, verifiable proofs of storage.\n",
    "\n",
    "### Possible Extras\n",
    "* **Figures**\n",
    "  1. CAS layout diagram: HCS → shard path → `.nhash` file with payload + footer.\n",
    "  2. Self-healing process: missing schema detected → rebuilt from footer.\n",
    "* **Pseudocode:** Rebuild algorithm from footers; schema resolution process.\n",
    "* **Invariant box:**\n",
    "   * *Deterministic address:* object path = HCS prefix.\n",
    "   * *Self-healing:* given corpus footers, all indices reconstructable without global scan.\n",
    "   * *Consistency:* rebuilt paths match original bit-for-bit.\n",
    "* **Artifact checklist:** Filing corpus, pipeline scripts, verification harness, benchmark environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee586a1-195f-4030-b3dc-10241e4b01c5",
   "metadata": {},
   "source": [
    "## BlockPRF Classes - (Pseudorandom Function) strategy”)\n",
    "**Encapsulates the hash functions supported by the HashChainStream. Built-in strategies implement a common BlockPRF interface and all emit 64-bit integers:**\n",
    "\n",
    "* **Blake2bPRF**\n",
    " * Stdlib hashlib.blake2b(digest_size=8)\n",
    " * Options: mac_key (keyed/MAC), person (≤16 bytes personalization)\n",
    "\n",
    "* **Blake3PRF**\n",
    " * Requires pip install blake3\n",
    " * Options: mac_key (exactly 32 bytes, keyed mode)\n",
    "\n",
    "* **XXH3PRF**\n",
    " * Requires pip install xxhash\n",
    " * Options: mac_key used as secret via set_secret\n",
    "\n",
    "* **XXH64PRF**\n",
    " * Requires pip install xxhash\n",
    " * Options: mac_key used to derive a 64-bit seed with blake2b(person=b\"HSEED\", digest_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b10486b9-689d-43c0-8be7-7aff64a29ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hash_chain_stream.py\n",
    "#from __future__ import annotations\n",
    "\n",
    "import hashlib\n",
    "import struct\n",
    "from abc import ABC, abstractmethod\n",
    "from itertools import islice\n",
    "from typing import Optional, Callable, Union\n",
    "\n",
    "# -------------------------------\n",
    "# PRF Strategy Interfaces & Impl\n",
    "# -------------------------------\n",
    "\n",
    "class BlockPRF(ABC):\n",
    "    \"\"\"\n",
    "    Strategy interface for per-block pseudo-random functions that emit 64-bit integers.\n",
    "\n",
    "    A BlockPRF MUST be deterministic for a fixed configuration and MUST map\n",
    "    an unsigned 64-bit counter `i` to an integer in [0, 2**64 - 1].\n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def __call__(self, i: int) -> int:  # returns 64-bit int\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # Small shared validator to keep error messages consistent (optional to use)\n",
    "    @staticmethod\n",
    "    def _validate_index(i: int) -> None:\n",
    "        if i < 0 or i > 0xFFFFFFFFFFFFFFFF:\n",
    "            raise OverflowError(\"block index out of 64-bit range (expected 0..2**64-1)\")\n",
    "\n",
    "\n",
    "class Blake2bPRF(BlockPRF):\n",
    "    \"\"\"\n",
    "    64-bit PRF using hashlib.blake2b with optional `key` (MAC) and `person` (<=16 bytes).\n",
    "\n",
    "    Output is the 8-byte digest interpreted big-endian as an integer.\n",
    "    \"\"\"\n",
    "    def __init__(self, key_bytes: bytes, mac_key: Optional[bytes] = None, person: Optional[bytes] = b\"HCv1\"):\n",
    "        if person is not None and len(person) > 16:\n",
    "            raise ValueError(\"blake2b personalization must be <= 16 bytes\")\n",
    "        base = hashlib.blake2b(digest_size=8, person=person or b\"\", key=(mac_key or b\"\"))\n",
    "        base.update(key_bytes)\n",
    "        self._base = base\n",
    "\n",
    "    def __call__(self, i: int) -> int:\n",
    "        self._validate_index(i)\n",
    "        h = self._base.copy()\n",
    "        h.update(struct.pack(\">Q\", i))\n",
    "        return int.from_bytes(h.digest(), \"big\")\n",
    "\n",
    "\n",
    "class Blake3PRF(BlockPRF):\n",
    "    \"\"\"\n",
    "    64-bit PRF using BLAKE3. Requires `pip install blake3`.\n",
    "\n",
    "    - If `mac_key` is provided, it MUST be exactly 32 bytes (BLAKE3 keyed mode).\n",
    "    - Output is 8 bytes from `digest(length=8)` interpreted big-endian.\n",
    "    \"\"\"\n",
    "    def __init__(self, key_bytes: bytes, mac_key: Optional[bytes] = None):\n",
    "        try:\n",
    "            import blake3  # type: ignore\n",
    "        except ImportError as e:\n",
    "            raise ImportError(\"blake3 not installed. `pip install blake3`\") from e\n",
    "        if mac_key is not None and len(mac_key) != 32:\n",
    "            raise ValueError(\"blake3 mac_key must be exactly 32 bytes\")\n",
    "        hasher = blake3.blake3(key=mac_key) if mac_key is not None else blake3.blake3()\n",
    "        hasher.update(key_bytes)\n",
    "        self._base = hasher\n",
    "\n",
    "    def __call__(self, i: int) -> int:\n",
    "        self._validate_index(i)\n",
    "        h = self._base.copy()\n",
    "        h.update(struct.pack(\">Q\", i))\n",
    "        return int.from_bytes(h.digest(length=8), \"big\")\n",
    "\n",
    "\n",
    "class XXH3PRF(BlockPRF):\n",
    "    \"\"\"\n",
    "    64-bit PRF using xxhash.xxh3_64. Requires `pip install xxhash`.\n",
    "\n",
    "    - If `mac_key` is provided, it is used as a 'secret' via set_secret().\n",
    "    - Output is `intdigest()` which is a 64-bit integer.\n",
    "    \"\"\"\n",
    "    def __init__(self, key_bytes: bytes, mac_key: Optional[bytes] = None):\n",
    "        try:\n",
    "            import xxhash  # type: ignore\n",
    "        except ImportError as e:\n",
    "            raise ImportError(\"xxhash not installed. `pip install xxhash`\") from e\n",
    "        self._xxhash = __import__(\"xxhash\")\n",
    "        self._key_bytes = key_bytes\n",
    "        self._secret = mac_key  # may be None\n",
    "\n",
    "    def __call__(self, i: int) -> int:\n",
    "        self._validate_index(i)\n",
    "        h = self._xxhash.xxh3_64()\n",
    "        if self._secret is not None:\n",
    "            h.reset()\n",
    "            h.set_secret(self._secret)\n",
    "        h.update(self._key_bytes)\n",
    "        h.update(struct.pack(\">Q\", i))\n",
    "        return h.intdigest()\n",
    "\n",
    "\n",
    "class XXH64PRF(BlockPRF):\n",
    "    \"\"\"\n",
    "    64-bit PRF using xxhash.xxh64 with a 64-bit seed.\n",
    "\n",
    "    - Requires `pip install xxhash`.\n",
    "    - If `mac_key` is provided, the seed is derived as:\n",
    "      blake2b(mac_key, digest_size=8, person=b\"HSEED\") big-endian.\n",
    "      Otherwise seed = 0 (unkeyed).\n",
    "    \"\"\"\n",
    "    def __init__(self, key_bytes: bytes, mac_key: Optional[bytes] = None):\n",
    "        try:\n",
    "            import xxhash  # type: ignore\n",
    "        except ImportError as e:\n",
    "            raise ImportError(\"xxhash not installed. `pip install xxhash`\") from e\n",
    "        self._xxhash = __import__(\"xxhash\")\n",
    "        if mac_key is not None:\n",
    "            seed = int.from_bytes(\n",
    "                hashlib.blake2b(mac_key, digest_size=8, person=b\"HSEED\").digest(),\n",
    "                \"big\"\n",
    "            )\n",
    "        else:\n",
    "            seed = 0\n",
    "        self._seed = seed\n",
    "        self._key_bytes = key_bytes\n",
    "\n",
    "    def __call__(self, i: int) -> int:\n",
    "        self._validate_index(i)\n",
    "        h = self._xxhash.xxh64(seed=self._seed)\n",
    "        h.update(self._key_bytes)\n",
    "        h.update(struct.pack(\">Q\", i))\n",
    "        return h.intdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832a856b-e72c-4145-9398-6443ffaeef59",
   "metadata": {},
   "source": [
    "# Class `HashChainStream`\n",
    "\n",
    "**An infinite, deterministic stream of entropy, emitted in nibbles or blocks**\n",
    "\n",
    "- Consume as much as you need, pause, then continue later — the sequence is repeatable.  \n",
    "- Output is **multi-seed, counter-driven, prefix-stable**, and effectively infinite.  \n",
    "- Designed for reproducibility and composability in radix maps and hash-chain structures.  \n",
    "\n",
    "---\n",
    "\n",
    "## 128-Bit Stream Example\n",
    "\n",
    "- **128 bits = 32 nibbles**  \n",
    "  - The stream first emits 16 nibbles from block `i = 0` (a 64-bit digest).  \n",
    "  - It then increments the counter (`i → 1`) to produce the next 64-bit block (another 16 nibbles).  \n",
    "  - So 128 bits corresponds to two consecutive 64-bit blocks (`i = 0` and `i = 1`).  \n",
    "\n",
    "- **Prefix stability & infinite length**  \n",
    "  Each block is derived from the same base state plus a monotone counter `i`:\n",
    "  * block_0 = H(key || 0)\n",
    "  * block_1 = H(key || 1)\n",
    "  * block_2 = H(key || 2)\n",
    "\n",
    "You can always extend the sequence by requesting higher `i`.  \n",
    "Previously emitted bits never change.  \n",
    "With a 64-bit counter you have up to `2^64` blocks → practically unbounded output.  \n",
    "\n",
    "---\n",
    "\n",
    "## Notes\n",
    "\n",
    "- **MSB-first ordering**: nibbles are taken from shifts `60, 56, …, 0` within each 64-bit block.  \n",
    "- **Determinism**: reproducibility depends on consistent key encoding. By default `repr(key).encode(\"utf-8\")` is used; for stricter semantics, pass a custom encoder.  \n",
    "- **Domain separation**: the personalization string (default `b\"HCSv1\"`) ensures separation from other BLAKE2b uses.  \n",
    "- **Performance/throughput**: for stronger independence or higher speed, increase `digest_size` or swap in a faster PRF (e.g., BLAKE3 or xxHash) without changing the counter pattern.  \n",
    "\n",
    "---\n",
    "\n",
    "## Key Encoding\n",
    "\n",
    "- `str` → UTF-8  \n",
    "- `bytes | bytearray | memoryview` → used as-is  \n",
    "- For structured keys or cross-language reproducibility, pass `encode_key=callable` to produce canonical bytes.  \n",
    "\n",
    "---\n",
    "\n",
    "## API Overview\n",
    "\n",
    "- **`block(i: int) -> int`**  \n",
    "Return the 64-bit block at counter index `i`.\n",
    "\n",
    "- **`nibble(d: int) -> int`**  \n",
    "Return the `d`-th nibble (`0..15`) MSB-first.\n",
    "\n",
    "- **`iter_blocks(start=0)`**  \n",
    "Infinite generator of 64-bit blocks.\n",
    "\n",
    "- **`iter_nibbles(start=0)`**  \n",
    "Infinite generator of 4-bit nibbles.\n",
    "\n",
    "- **`take_blocks(n, start=0)`**  \n",
    "Materialize `n` blocks into a list.\n",
    "\n",
    "- **`take_nibbles(n, start=0)`**  \n",
    "Materialize `n` nibbles into a list.\n",
    "\n",
    "- **`take_hex(n_nibbles, start=0)`**  \n",
    "Render `n_nibbles` as a compact hex string (1 hex digit per nibble).\n",
    "\n",
    "- **`take_bytes(n_bytes, start=0)`**  \n",
    "Render `n_bytes` as raw bytes (2 nibbles per byte).\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1ea94bf-4d9d-4335-adfa-c31c7658bdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "from typing import Optional, Callable, Union\n",
    "\n",
    "class HashChainStream:\n",
    "    \"\"\"\n",
    "    Deterministic stream of 64-bit blocks and MSB-first 4-bit nibbles,\n",
    "    driven by a pluggable PRF (Pseudorandom Function).\n",
    "\n",
    "    Overview\n",
    "    --------\n",
    "    For a given configuration, each block is computed as::\n",
    "\n",
    "        PRF( key_bytes || counter64_be(i) ) → 64-bit integer\n",
    "\n",
    "    where:\n",
    "      • ``key_bytes`` is a canonicalized byte encoding of the key  \n",
    "      • ``counter64_be(i)`` is the 8-byte big-endian encoding of block index ``i``\n",
    "\n",
    "    Each 64-bit block yields 16 nibbles, extracted **MSB-first** (bits 63..60,\n",
    "    59..56, …, 3..0). This convention preserves big-endian concatenation\n",
    "    semantics when rendering to hex or assembling larger integers.\n",
    "\n",
    "    Hybrid API\n",
    "    ----------\n",
    "    - **Simple mode**:\n",
    "        Pass ``algo=\"blake2b\" | \"blake3\" | \"xxh3\" | \"xxh64\"`` and optional\n",
    "        ``mac_key`` / ``person``. The stream constructs the PRF internally\n",
    "        using :meth:`HashChainStream.make_prf`.\n",
    "\n",
    "    - **Advanced mode**:\n",
    "        Pass an explicit ``prf=BlockPRF(...)`` instance (or any callable\n",
    "        ``i:int -> int``). This allows full control, custom PRFs, and easy\n",
    "        extensibility without changing the `HashChainStream` API.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key : str | bytes | bytearray | memoryview | object\n",
    "        Identifier for the stream. Canonicalized to bytes. For custom\n",
    "        encodings (e.g., tuples, structs), supply ``encode_key=``.\n",
    "    prf : BlockPRF | callable, optional\n",
    "        Explicit PRF strategy. If provided, ``algo`` / ``mac_key`` / ``person``\n",
    "        are ignored.\n",
    "    algo : str, default \"blake2b\"\n",
    "        Built-in PRF to use when ``prf`` is not given.\n",
    "    mac_key : bytes, optional\n",
    "        Keyed/MAC mode material. Semantics depend on the chosen algorithm.\n",
    "    person : bytes, optional\n",
    "        BLAKE2b-only personalization tag (≤16 bytes).\n",
    "    encode_key : callable, optional\n",
    "        Custom encoder mapping ``key`` → bytes.\n",
    "\n",
    "    Guarantees\n",
    "    ----------\n",
    "    - **Deterministic**: identical outputs for the same configuration across\n",
    "      processes/machines.\n",
    "    - **Prefix-stable**: earlier blocks/nibbles never change when requesting\n",
    "      later indices.\n",
    "    - **Domain-separated**: keyed/MAC modes and personalization allow multiple\n",
    "      independent streams even for identical keys.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - ``block(i)`` → single 64-bit block\n",
    "    - ``nibble(d)`` → single 4-bit nibble\n",
    "    - Iterators (:meth:`iter_blocks`, :meth:`iter_nibbles`) are infinite; use\n",
    "      with care.\n",
    "    - Convenience collectors (:meth:`take_blocks`, :meth:`take_nibbles`,\n",
    "      :meth:`take_hex`, :meth:`take_bytes`) materialize finite slices.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> hc = HashChainStream(\"orders:2025\")        # simple blake2b stream\n",
    "    >>> hc.block(0)\n",
    "    7091458127735587405\n",
    "    >>> hc.take_hex(8)\n",
    "    '62b38f9d'\n",
    "\n",
    "    >>> from hash_chain_stream import Blake2bPRF\n",
    "    >>> prf = Blake2bPRF(b\"user-key\", mac_key=b\"secret\", person=b\"HCradix\")\n",
    "    >>> hc = HashChainStream(b\"user-key\", prf=prf) # advanced mode\n",
    "    >>> hc.nibble(0)\n",
    "    6\n",
    "    \"\"\"\n",
    "    __slots__ = (\"_prf_block\",)\n",
    "\n",
    "    DEFAULT_PERSON = b\"HCv1\"\n",
    "    NIBBLES_PER_BLOCK = 16  # 64 bits / 4 bits\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        key: Union[str, bytes, bytearray, memoryview, object],\n",
    "        *,\n",
    "        # hybrid knobs\n",
    "        prf: Optional[BlockPRF] = None,\n",
    "        algo: str = \"blake2b\",                  # used iff prf is None\n",
    "        mac_key: Optional[bytes] = None,        # used iff prf is None\n",
    "        person: Optional[bytes] = DEFAULT_PERSON,  # blake2b-only, used iff prf is None\n",
    "        encode_key: Optional[Callable[[object], bytes]] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize a **HashChainStream** — a deterministic stream of 64-bit blocks and\n",
    "        MSB-first 4-bit nibbles.\n",
    "    \n",
    "        Each block is computed as a PRF (Pseudorandom Function) over:\n",
    "        ```\n",
    "        PRF( key_bytes || counter64_be(i) )  →  64-bit integer\n",
    "        ```\n",
    "        where `counter64_be(i)` is the big-endian 8-byte encoding of the block index `i`.\n",
    "        Downstream methods read nibbles **MSB-first** from each 64-bit block\n",
    "        (bits 63..60, 59..56, …, 3..0), preserving big-endian concatenation semantics.\n",
    "    \n",
    "        ### Hybrid construction\n",
    "        You may provide either:\n",
    "        1) **`prf`** — a `BlockPRF` strategy instance (or any callable `i:int -> 64-bit int`).\n",
    "           If supplied, it **takes precedence** and `algo` / `mac_key` / `person` are ignored.\n",
    "        2) **`algo`** + optional `mac_key` / `person` — the stream will construct a built-in PRF\n",
    "           internally via `HashChainStream.make_prf()`.\n",
    "    \n",
    "        ### Parameters\n",
    "        - **key** (`str | bytes | bytearray | memoryview | object`):\n",
    "          Identifier for the stream. If `encode_key` is **None**:\n",
    "          - `str` → UTF-8 encoded\n",
    "          - `bytes|bytearray|memoryview` → used as-is\n",
    "          - otherwise, a `TypeError` is raised  \n",
    "          Provide `encode_key=callable` for custom canonical encodings (e.g., struct-packed tuples).\n",
    "    \n",
    "        - **prf** (`Optional[BlockPRF]`, default: `None`):\n",
    "          A per-block PRF strategy. Must be deterministic and map `i ∈ [0, 2**64-1]`\n",
    "          to a 64-bit integer. If given, overrides `algo`/`mac_key`/`person`.\n",
    "    \n",
    "        - **algo** (`str`, default: `\"blake2b\"`; used iff `prf` is `None`):\n",
    "          Name of a built-in PRF to construct:\n",
    "          - `\"blake2b\"` — stdlib `hashlib.blake2b`, 8-byte digest\n",
    "          - `\"blake3\"`  — requires `pip install blake3`\n",
    "          - `\"xxh3\"`    — requires `pip install xxhash`\n",
    "          - `\"xxh64\"`   — requires `pip install xxhash`\n",
    "    \n",
    "        - **mac_key** (`bytes | None`, default: `None`; used iff `prf` is `None`):\n",
    "          Enables keyed/MAC mode where supported:\n",
    "          - **BLAKE2b**: passed as `key=` to `hashlib.blake2b`\n",
    "          - **BLAKE3**: must be **exactly 32 bytes**\n",
    "          - **XXH3**: used as **secret** via `set_secret(mac_key)`\n",
    "          - **XXH64**: used to derive a 64-bit **seed** with\n",
    "            `blake2b(person=b\"HSEED\", digest_size=8)`\n",
    "    \n",
    "        - **person** (`bytes | None`, default: `DEFAULT_PERSON`; used iff `prf` is `None`):\n",
    "          **BLAKE2b-only** personalization (domain separation), **≤ 16 bytes**.\n",
    "          Ignored for other algorithms. Use `None` to omit.\n",
    "    \n",
    "        - **encode_key** (`Callable[[object], bytes] | None`, default: `None`):\n",
    "          Optional converter `key → bytes`. Use this to enforce cross-language stability\n",
    "          (e.g., canonical CBOR/MsgPack, `struct.pack` over fixed layouts). If provided,\n",
    "          it overrides the default `str/bytes` handling above.\n",
    "    \n",
    "        ### Determinism & prefix-stability\n",
    "        For a fixed `(key_bytes, prf)` (or fixed `(key_bytes, algo, mac_key, person)`):\n",
    "        - **Deterministic across processes/machines**\n",
    "        - **Prefix-stable** — requesting more blocks never changes earlier blocks\n",
    "    \n",
    "        ### Cross-language reproducibility\n",
    "        To guarantee identical streams across languages/runtimes:\n",
    "        1. Fix `algo`, `mac_key`, and `person` (for BLAKE2b).\n",
    "        2. Use a **stable byte encoding** for `key` (`encode_key`).\n",
    "        3. Treat the counter as **big-endian 64-bit** (`>Q`).\n",
    "    \n",
    "        ### Performance\n",
    "        - Per block: one PRF evaluation over the 8-byte counter (on a pre-seeded state\n",
    "          for BLAKE2b/BLAKE3), plus minimal Python overhead.\n",
    "        - Nibble helpers amortize to **1 block per 16 nibbles**.\n",
    "    \n",
    "        ### Raises\n",
    "        - **TypeError** — `key` is not `str`/bytes-like and `encode_key` is not provided;\n",
    "          or `prf` is neither a `BlockPRF` nor a callable `(int)->int`.\n",
    "        - **ValueError**\n",
    "          - Unsupported `algo`\n",
    "          - `person` length > 16 with BLAKE2b\n",
    "          - `blake3` with `mac_key` length ≠ 32\n",
    "        - **ImportError** — chosen algorithm requires a missing dependency (`blake3`, `xxhash`).\n",
    "        - **OverflowError** — when a caller later requests a block index outside `[0, 2**64-1]`.\n",
    "    \n",
    "        ### Examples\n",
    "        ```python\n",
    "        # Simple: internal PRF (default blake2b + personalization)\n",
    "        hc = HashChainStream(\"orders:2025\")\n",
    "    \n",
    "        # Keyed BLAKE3 (32-byte key)\n",
    "        hc = HashChainStream(\"k\", algo=\"blake3\", mac_key=b\"\\x00\"*32)\n",
    "    \n",
    "        # XXH3 with secret\n",
    "        hc = HashChainStream(b\"k\", algo=\"xxh3\", mac_key=b\"xxh3-secret\")\n",
    "    \n",
    "        # Advanced: explicit PRF strategy\n",
    "        prf = Blake2bPRF(b\"user-key\", mac_key=b\"secret\", person=b\"HCradix\")\n",
    "        hc = HashChainStream(b\"user-key\", prf=prf)\n",
    "        ```\n",
    "    \n",
    "        ### See also\n",
    "        - `HashChainStream.make_prf()` — static factory for built-in PRFs\n",
    "        - `Blake2bPRF`, `Blake3PRF`, `XXH3PRF`, `XXH64PRF` — PRF strategy classes\n",
    "        - `block`, `nibble`, `iter_blocks`, `iter_nibbles`, `take_hex`, `take_bytes`\n",
    "        \"\"\"\n",
    "        # Canonicalize key → bytes\n",
    "        if encode_key is None:\n",
    "            if isinstance(key, (bytes, bytearray, memoryview)):\n",
    "                key_bytes = bytes(key)\n",
    "            elif isinstance(key, str):\n",
    "                key_bytes = key.encode(\"utf-8\")\n",
    "            else:\n",
    "                # allow advanced users to provide custom encoders for structured keys\n",
    "                raise TypeError(\"key must be str or bytes-like; supply encode_key= for custom types\")\n",
    "        else:\n",
    "            key_bytes = encode_key(key)\n",
    "\n",
    "        if prf is not None:\n",
    "            if not isinstance(prf, BlockPRF):\n",
    "                # accommodate duck-typed strategies that implement __call__\n",
    "                if not callable(prf):\n",
    "                    raise TypeError(\"prf must be a BlockPRF or callable(i:int)->64-bit int\")\n",
    "            self._prf_block = prf  # type: ignore[assignment]\n",
    "        else:\n",
    "            self._prf_block = self.make_prf(algo, key_bytes, mac_key=mac_key, person=person)\n",
    "\n",
    "    @staticmethod\n",
    "    def make_prf(\n",
    "        algo: str,\n",
    "        key_bytes: bytes,\n",
    "        *,\n",
    "        mac_key: bytes | None = None,\n",
    "        person: bytes | None = DEFAULT_PERSON,\n",
    "    ) -> \"BlockPRF\":\n",
    "        \"\"\"\n",
    "        Build a **PRF (Pseudorandom Function) strategy** for 64-bit block generation.\n",
    "    \n",
    "        This factory returns a `BlockPRF` instance that maps a 64-bit counter `i`\n",
    "        (big-endian, 0 ≤ i ≤ 2**64−1) to a deterministic 64-bit integer:\n",
    "        ```\n",
    "        PRF(key_bytes || counter64_be(i)) → int in [0, 2**64 - 1]\n",
    "        ```\n",
    "        The choice of algorithm affects performance and security properties but not\n",
    "        the interface or output width.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        algo : str\n",
    "            Name of the built-in PRF to construct. One of:\n",
    "            - `\"blake2b\"` — stdlib hashlib; digest truncated to 8 bytes.\n",
    "            - `\"blake3\"`  — requires `pip install blake3`; digest truncated to 8 bytes.\n",
    "            - `\"xxh3\"`    — requires `pip install xxhash`; 64-bit xxh3.\n",
    "            - `\"xxh64\"`   — requires `pip install xxhash`; 64-bit xxh64 with seed.\n",
    "        key_bytes : bytes\n",
    "            Canonicalized stream key as bytes. For cross-language reproducibility,\n",
    "            ensure callers provide a stable encoding (e.g., UTF-8 string, struct-packed tuple).\n",
    "        mac_key : bytes | None, optional\n",
    "            Enables keyed/MAC mode where supported:\n",
    "            - **blake2b**: passed as `key=` to `hashlib.blake2b`.\n",
    "            - **blake3**: must be **exactly 32 bytes** (BLAKE3 keyed mode).\n",
    "            - **xxh3**: used as a **secret** via `set_secret(mac_key)`.\n",
    "            - **xxh64**: used to derive a 64-bit **seed** by hashing `mac_key` with\n",
    "              BLAKE2b(person=`b\"HSEED\"`, digest_size=8). If `None`, seed=0 (unkeyed).\n",
    "        person : bytes | None, optional\n",
    "            **BLAKE2b-only** personalization (domain separation), must be **≤ 16 bytes**.\n",
    "            Ignored for other algorithms. Use `None` to omit.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        BlockPRF\n",
    "            A callable strategy implementing `__call__(i:int) -> int` that yields 64-bit blocks.\n",
    "    \n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If `algo` is unsupported, `person` is >16 bytes for blake2b,\n",
    "            or `mac_key` length is invalid for the chosen algorithm (e.g., blake3 ≠ 32 bytes).\n",
    "        ImportError\n",
    "            If the selected algorithm requires a missing third-party package (`blake3`, `xxhash`).\n",
    "    \n",
    "        Determinism & Prefix-Stability\n",
    "        ------------------------------\n",
    "        For a fixed `(algo, key_bytes, mac_key, person)`, the returned PRF is deterministic\n",
    "        across processes/machines and prefix-stable (earlier blocks do not change when later\n",
    "        indices are queried).\n",
    "    \n",
    "        Notes\n",
    "        -----\n",
    "        - This factory does **not** mutate `key_bytes`; the caller is responsible for providing\n",
    "          a canonical byte representation.\n",
    "        - Security: `blake2b`/`blake3` (keyed) behave as PRFs; `xxh3/xxh64` are **not** cryptographic\n",
    "          and should be chosen for speed/domain separation, not adversarial security.\n",
    "    \n",
    "        Examples\n",
    "        --------\n",
    "        >>> # Via the class (recommended)\n",
    "        >>> prf = HashChainStream.make_prf(\"blake2b\", b\"user-key\", person=b\"HCv1\")\n",
    "        >>> prf(0)  # 64-bit int\n",
    "    \n",
    "        >>> # Keyed BLAKE3 (32-byte key)\n",
    "        >>> prf = HashChainStream.make_prf(\"blake3\", b\"user-key\", mac_key=b\"\\\\x00\"*32)\n",
    "    \n",
    "        >>> # XXH3 with secret\n",
    "        >>> prf = HashChainStream.make_prf(\"xxh3\", b\"user-key\", mac_key=b\"secret\")\n",
    "    \n",
    "        >>> # XXH64 with seed derived from mac_key\n",
    "        >>> prf = HashChainStream.make_prf(\"xxh64\", b\"user-key\", mac_key=b\"seed-material\")\n",
    "        \"\"\"\n",
    "        a = algo.lower()\n",
    "        if a == \"blake2b\":\n",
    "            return Blake2bPRF(key_bytes, mac_key=mac_key, person=person)\n",
    "        if a == \"blake3\":\n",
    "            return Blake3PRF(key_bytes, mac_key=mac_key)\n",
    "        if a == \"xxh3\":\n",
    "            return XXH3PRF(key_bytes, mac_key=mac_key)\n",
    "        if a == \"xxh64\":\n",
    "            return XXH64PRF(key_bytes, mac_key=mac_key)\n",
    "        raise ValueError(\"Unsupported algo. Use 'blake2b', 'blake3', 'xxh3', or 'xxh64'.\")\n",
    "        \n",
    "    # -------- Core --------\n",
    "\n",
    "    def block(self, i: int) -> int:\n",
    "        \"\"\"\n",
    "        Return the 64-bit block for counter index `i`.\n",
    "    \n",
    "        This is a thin, intentional wrapper around the internal PRF callable\n",
    "        (`self._prf_block`). Keeping `block()` as the public entry point provides:\n",
    "        - A **single choke point** for validation, micro-caching, or instrumentation\n",
    "          (timing/metrics/logging) without changing call sites.\n",
    "        - **API clarity** for consumers (reads better than calling a private field).\n",
    "        - **Future-proofing** if the block derivation ever needs extra behavior.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        i : int\n",
    "            Zero-based 64-bit counter index (0 ≤ i ≤ 2**64−1). Interpreted as\n",
    "            big-endian when combined with the key inside the PRF.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The 64-bit block value in the range [0, 2**64 − 1].\n",
    "    \n",
    "        Raises\n",
    "        ------\n",
    "        OverflowError\n",
    "            If `i` falls outside the supported 64-bit range (when validation is enabled).\n",
    "        ValueError\n",
    "            If a negative index is supplied (when validation is enabled).\n",
    "    \n",
    "        Notes\n",
    "        -----\n",
    "        - By default this method forwards to `self._prf_block(i)`. Projects that\n",
    "          need extra guarantees can add `BlockPRF._validate_index(i)` calls or a\n",
    "          one-entry cache here with negligible overhead.\n",
    "        \"\"\"\n",
    "        return self._prf_block(i)  # type: ignore[misc]\n",
    "\n",
    "    def nibble(self, d: int) -> int:\n",
    "        \"\"\"\n",
    "        Return the **d-th 4-bit nibble** (0..15) from the stream, read **MSB-first**\n",
    "        within each 64-bit block.\n",
    "    \n",
    "        The hash-chain stream is partitioned into blocks of 64 bits, each yielding\n",
    "        16 nibbles. This accessor computes the block index and intra-block offset,\n",
    "        derives the block via :meth:`block`, and extracts the nibble from bit\n",
    "        groups **63..60, 59..56, …, 3..0** (big-endian/MSB-first). The MSB-first\n",
    "        convention ensures that concatenating successive nibbles produces the same\n",
    "        big-endian value you would get by rendering blocks to hex and joining them.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        d : int\n",
    "            Zero-based **global nibble index** (``d >= 0``).\n",
    "            - ``d // 16`` selects the 64-bit block index.\n",
    "            - ``d % 16`` selects the nibble position within that block.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The requested nibble as an integer in ``[0, 15]``.\n",
    "    \n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If ``d`` is negative.\n",
    "        OverflowError\n",
    "            If the derived block index exceeds the supported 64-bit counter range\n",
    "            (when :meth:`block` validates indices).\n",
    "    \n",
    "        Performance\n",
    "        -----------\n",
    "        O(1) per call: one 64-bit block derivation via :meth:`block` and a constant-time\n",
    "        shift-and-mask. For many consecutive nibbles, prefer :meth:`iter_nibbles` or\n",
    "        :meth:`take_hex` to amortize block computations (≈ one block per 16 nibbles).\n",
    "    \n",
    "        Examples\n",
    "        --------\n",
    "        >>> hc = HashChainStream(\"demo\")\n",
    "        >>> hc.nibble(0)          # first nibble of block 0 (bits 63..60)\n",
    "        6\n",
    "        >>> hc.nibble(15)         # last nibble of block 0 (bits 3..0)\n",
    "        13\n",
    "        >>> hc.nibble(16)         # first nibble of block 1\n",
    "        2\n",
    "        \"\"\"\n",
    "        if d < 0:\n",
    "            raise ValueError(\"nibble index must be non-negative\")\n",
    "        blk_index, off = divmod(d, self.NIBBLES_PER_BLOCK)\n",
    "        blk = self.block(blk_index)\n",
    "        return (blk >> (60 - 4 * off)) & 0xF\n",
    "\n",
    "    # -------- Iteration --------\n",
    "\n",
    "    def iter_blocks(self, start: int = 0):\n",
    "        \"\"\"\n",
    "        Yield successive 64-bit blocks from the stream, starting at block index `start`.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        start : int, optional\n",
    "            Zero-based block index to begin iteration (default: 0).\n",
    "            Must be non-negative.\n",
    "    \n",
    "        Yields\n",
    "        ------\n",
    "        int\n",
    "            Deterministic 64-bit block values in the range ``[0, 2**64 - 1]``,\n",
    "            for indices ``start, start+1, start+2, …``.\n",
    "    \n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If `start` is negative.\n",
    "        OverflowError\n",
    "            If an index exceeds the supported 64-bit counter range (when validated).\n",
    "    \n",
    "        Notes\n",
    "        -----\n",
    "        - Each block is computed as ``PRF(key_bytes || counter64_be(i))``.\n",
    "        - This generator is infinite; consume with care.\n",
    "        - Prefix stability: requesting more blocks never alters earlier values.\n",
    "    \n",
    "        Examples\n",
    "        --------\n",
    "        >>> hc = HashChainStream(\"demo\")\n",
    "        >>> it = hc.iter_blocks()\n",
    "        >>> [next(it) for _ in range(2)]\n",
    "        [11346205187653249752, 7246645061858420513]\n",
    "    \n",
    "        >>> list(islice(hc.iter_blocks(start=5), 3))\n",
    "        [block_5, block_6, block_7]\n",
    "        \"\"\"\n",
    "        if start < 0:\n",
    "            raise ValueError(\"start must be non-negative\")\n",
    "        i = start\n",
    "        while True:\n",
    "            yield self.block(i)\n",
    "            i += 1\n",
    "\n",
    "    def iter_nibbles(self, start: int = 0):\n",
    "        \"\"\"\n",
    "        Yield successive 4-bit nibbles (MSB-first) from the stream,\n",
    "        starting at global nibble index `start`.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        start : int, optional\n",
    "            Zero-based global nibble index to begin iteration (default: 0).\n",
    "            Must be non-negative.\n",
    "            - ``start = 0`` → first nibble of block 0 (bits 63..60).\n",
    "            - ``start = 15`` → last nibble of block 0 (bits 3..0).\n",
    "            - ``start = 16`` → first nibble of block 1.\n",
    "    \n",
    "        Yields\n",
    "        ------\n",
    "        int\n",
    "            The next 4-bit nibble as an integer in ``[0, 15]``.\n",
    "    \n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If `start` is negative.\n",
    "        OverflowError\n",
    "            If a derived block index exceeds the supported 64-bit counter range (when validated).\n",
    "    \n",
    "        Notes\n",
    "        -----\n",
    "        - Each block provides exactly 16 nibbles, extracted MSB-first.\n",
    "        - Iteration is infinite; consume with care.\n",
    "        - Efficient: one block computation per 16 yielded nibbles.\n",
    "    \n",
    "        Examples\n",
    "        --------\n",
    "        >>> hc = HashChainStream(\"demo\")\n",
    "        >>> it = hc.iter_nibbles()\n",
    "        >>> [next(it) for _ in range(5)]\n",
    "        [6, 14, 11, 2, 9]\n",
    "    \n",
    "        >>> list(islice(hc.iter_nibbles(start=18), 4))\n",
    "        [6, 15, 4, 8]\n",
    "        \"\"\"\n",
    "        if start < 0:\n",
    "            raise ValueError(\"start must be non-negative\")\n",
    "        i, off = divmod(start, self.NIBBLES_PER_BLOCK)\n",
    "        blk = self.block(i)\n",
    "        while True:\n",
    "            yield (blk >> (60 - 4 * off)) & 0xF\n",
    "            off += 1\n",
    "            if off == self.NIBBLES_PER_BLOCK:\n",
    "                i += 1\n",
    "                blk = self.block(i)\n",
    "                off = 0\n",
    "\n",
    "    # -------- Convenience --------\n",
    "\n",
    "    def take_blocks(self, n: int, start: int = 0):\n",
    "        \"\"\"\n",
    "        Collect a finite slice of 64-bit blocks from the stream.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        n : int\n",
    "            Number of blocks to return (``n >= 0``).\n",
    "        start : int, optional\n",
    "            Zero-based block index at which to begin (default: 0).\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        list[int]\n",
    "            A list of length ``n`` containing 64-bit integers, where\n",
    "            element ``k`` corresponds to block index ``start + k``.\n",
    "    \n",
    "        Examples\n",
    "        --------\n",
    "        >>> hc = HashChainStream(\"demo\")\n",
    "        >>> hc.take_blocks(2)\n",
    "        [8007281822324105533, 15007275084512821485]\n",
    "        \"\"\"\n",
    "        return list(islice(self.iter_blocks(start), n))\n",
    "\n",
    "    def take_nibbles(self, n: int, start: int = 0):\n",
    "        \"\"\"\n",
    "        Collect a finite slice of 4-bit nibbles from the stream.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        n : int\n",
    "            Number of nibbles to return (``n >= 0``).\n",
    "        start : int, optional\n",
    "            Zero-based global nibble index at which to begin (default: 0).\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        list[int]\n",
    "            A list of length ``n`` containing integers in ``[0, 15]``.\n",
    "    \n",
    "        Examples\n",
    "        --------\n",
    "        >>> hc = HashChainStream(\"demo\")\n",
    "        >>> hc.take_nibbles(5)\n",
    "        [6, 14, 11, 2, 9]\n",
    "        \"\"\"\n",
    "        return list(islice(self.iter_nibbles(start), n))\n",
    "\n",
    "    def take_hex(self, n_nibbles: int, start: int = 0) -> str:\n",
    "        \"\"\"\n",
    "        Render a slice of the stream as a compact hexadecimal string.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nibbles : int\n",
    "            Number of nibbles (hex digits) to emit. Must be non-negative.\n",
    "        start : int, optional\n",
    "            Zero-based global nibble index to begin from (default: 0).\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            A lowercase hex string of length ``n_nibbles`` representing the\n",
    "            selected nibbles, concatenated MSB-first.\n",
    "    \n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If `n_nibbles` or `start` is negative.\n",
    "    \n",
    "        Examples\n",
    "        --------\n",
    "        >>> hc = HashChainStream(\"demo\")\n",
    "        >>> hc.take_hex(8)          # 8 nibbles = 32 bits\n",
    "        '6eb29f78'\n",
    "        >>> hc.take_hex(8, start=16)\n",
    "        '2c6f48a1'\n",
    "        \"\"\"        \n",
    "        if n_nibbles < 0 or start < 0:\n",
    "            raise ValueError(\"n_nibbles and start must be non-negative\")\n",
    "        if n_nibbles == 0:\n",
    "            return \"\"\n",
    "        out = 0\n",
    "        remaining = n_nibbles\n",
    "        blk_i, off = divmod(start, self.NIBBLES_PER_BLOCK)\n",
    "        blk = self.block(blk_i)\n",
    "        while remaining:\n",
    "            take = min(remaining, self.NIBBLES_PER_BLOCK - off)\n",
    "            for k in range(off, off + take):\n",
    "                out = (out << 4) | ((blk >> (60 - 4 * k)) & 0xF)\n",
    "            remaining -= take\n",
    "            off += take\n",
    "            if off == self.NIBBLES_PER_BLOCK and remaining:\n",
    "                blk_i += 1\n",
    "                blk = self.block(blk_i)\n",
    "                off = 0\n",
    "        return f\"{out:0{n_nibbles}x}\"\n",
    "\n",
    "    def take_bytes(self, n_bytes: int, start_nibble: int = 0) -> bytes:\n",
    "        hx = self.take_hex(n_bytes * 2, start=start_nibble)\n",
    "        return bytes.fromhex(hx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d816b1-5dbb-463a-ac6e-7b71401f81dc",
   "metadata": {},
   "source": [
    "## HashChainStream Examples (Different Algorithms)\n",
    "\n",
    "The `HashChainStream` can be configured with different pseudorandom functions (PRFs).  \n",
    "Below are short demonstrations of each built-in algorithm.\n",
    "\n",
    "* All algorithms implement the same API, so you can swap PRFs without changing call sites.\n",
    "* Blocks are 64-bit integers, nibbles are extracted MSB-first.\n",
    "* Keyed/MAC modes add separation for multi-tenant or security-sensitive uses.\n",
    "\n",
    "\n",
    "> **Note:** `blake3` and `xxhash` must be installed for those examples (`pip install blake3 xxhash`).\n",
    "\n",
    "---\n",
    "\n",
    "### Blake2b (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6824fc2-df5b-4ac5-b75f-705ac6e8f62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blake2b blocks: [985851846286183629, 3423197683260989405, 3570893657644300491]\n",
      "Blake2b nibbles: [0, 13, 10, 14, 7, 3, 0, 7, 14, 11, 0, 13, 0, 12, 12, 13, 2, 15, 8, 1]\n"
     ]
    }
   ],
   "source": [
    "hc = HashChainStream(\"demo\", algo=\"blake2b\")\n",
    "\n",
    "# First three 64-bit blocks\n",
    "print(\"Blake2b blocks:\", hc.take_blocks(3))\n",
    "\n",
    "# First 20 nibbles (MSB-first)\n",
    "print(\"Blake2b nibbles:\", hc.take_nibbles(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763b7f19-1a6d-4fb9-b3f4-9dc6894d29f3",
   "metadata": {},
   "source": [
    "### Blake3 (requires blake3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bc50f65-b2d5-4369-a2b2-9c153ae24302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blake3 blocks: [15954229562184433180, 1054599622573350758, 16502147770193980363]\n",
      "Blake3 nibbles: [13, 13, 6, 8, 12, 15, 4, 1, 15, 7, 0, 9, 15, 14, 1, 12, 0, 14, 10, 2]\n"
     ]
    }
   ],
   "source": [
    "hc = HashChainStream(\"demo\", algo=\"blake3\")\n",
    "\n",
    "# First three 64-bit blocks\n",
    "print(\"Blake3 blocks:\", hc.take_blocks(3))\n",
    "\n",
    "# First 20 nibbles\n",
    "print(\"Blake3 nibbles:\", hc.take_nibbles(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f32045-3351-4cfa-98e9-2cd13edd7538",
   "metadata": {},
   "source": [
    "### XXH3 (requires xxhash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9179637-cdc4-4244-83c6-0039e4fe496b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XXH3 blocks: [9057493142847259777, 15285933752601757588, 7413832240767436247]\n",
      "XXH3 nibbles: [7, 13, 11, 2, 10, 14, 0, 6, 5, 6, 15, 2, 8, 4, 8, 1, 13, 4, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "hc = HashChainStream(\"demo\", algo=\"xxh3\")\n",
    "\n",
    "print(\"XXH3 blocks:\", hc.take_blocks(3))\n",
    "print(\"XXH3 nibbles:\", hc.take_nibbles(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8973a1-d116-4b58-8f67-b8f45a397a3c",
   "metadata": {},
   "source": [
    "### XXH64 (requires xxhash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86ad9b87-c2d1-478f-9a3c-55f64011852b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XXH64 blocks: [18183894411860892043, 10343756514337400774, 10311484311676788828]\n",
      "XXH64 nibbles: [15, 12, 5, 10, 2, 11, 10, 0, 0, 12, 2, 13, 13, 9, 8, 11, 8, 15, 8, 12]\n"
     ]
    }
   ],
   "source": [
    "hc = HashChainStream(\"demo\", algo=\"xxh64\")\n",
    "\n",
    "print(\"XXH64 blocks:\", hc.take_blocks(3))\n",
    "print(\"XXH64 nibbles:\", hc.take_nibbles(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83384f35-1bc3-4283-bf49-14c610acc478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeded XXH64 first block: 3158562142525797262\n"
     ]
    }
   ],
   "source": [
    "hc_seeded = HashChainStream(\"demo\", algo=\"xxh64\", mac_key=b\"seed-material\")\n",
    "print(\"Seeded XXH64 first block:\", hc_seeded.block(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cdf297-56f1-405d-99f3-c436f1173b9d",
   "metadata": {},
   "source": [
    "### Generate a hex hash code of any length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0b1b06b-caf6-48a7-be35-9de36d81c161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requested length: 1024 hex chars\n",
      "\n",
      "Hash code:\n",
      "438531a79d15055d9ae06358ac87a131d83f5ee5d00950b860c655db02b6562ecf7f56ee3c55af7f96cd5e056688ed173862bffd6acff2511b0f7144804071a599acd51f85ccf72e5de40f97f960022bdfc34991a5ffbee78cdeac5eb53f5b466c5144ad049c1683ba530a78f2fcd7233abb3ec8ce51d0164ea303968057fcece5e25049837efee1571a52dba22573643b96c2670b40edac384d607e01aec0e1b37e5abb52104ae1dd15bc43d53b982fdc266afb4db21da9d3d8646a0ba873a295a011ce7bb3f70b0c2e191d5d18e30b8b0cdd9825a541a8597ca64bd15276e55e2561b6ccecf0723316d79706c7ca7e20733784a829c15d206d3916a908f37bd70c2ffa07e70ea4129f76b034727db4779e5e6ad8b7401ba5918192faa5f7e31b9bd06c233e8c7161e41ff7a2cce7b5dd1d47614f13ad209e0399f04584374dda71c76b8681603e0f82ed116d2b9be80a6192c8824c2ef9cff128eda538a9714f9b324abc550a4331dcaf03e489887a1396c6fa3f8f0cbf29684c3db1afb6bb1756f482d62fdb76a5a46ac84fe96ca3ebf68d5dc695d49a0a15611c3d1d86ef06fa4a77c48a8f00f034381cebb60e371b74e1958666ac6aebcdad6bdf135158ba33433c03d22913c52a2f1b36f1d8203b23e66833f0aff4dd93f4b0f439059a4bd3016b32d803fbe9e2f2d4789b657f81976af90ac4e1bf4dabbe81e86a1639\n",
      "\n",
      "Actual length: 1024\n"
     ]
    }
   ],
   "source": [
    "# --- configure ---\n",
    "key = \"demo-key\"\n",
    "algo = \"xxh3\"   # or \"blake3\", \"xxh3\", \"xxh64\"\n",
    "hash_code_length = 1024  # number of hex characters you want\n",
    "\n",
    "# --- build stream ---\n",
    "hc = HashChainStream(key, algo=algo)\n",
    "\n",
    "# --- generate hex code ---\n",
    "hash_code = hc.take_hex(hash_code_length)\n",
    "\n",
    "print(f\"Requested length: {hash_code_length} hex chars\")\n",
    "print(f\"\\nHash code:\\n{hash_code}\")\n",
    "print(f\"\\nActual length: {len(hash_code)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dd088a-48a6-4b06-9d1e-a1aab1829e35",
   "metadata": {},
   "source": [
    "## RadixHashChainMap Using HCA (Hash Chain Augmentation)\n",
    "\n",
    "RadixHashChainMap is a HAMT-style radix trie keyed by MSB-first nibbles from a hash-chain (PRF) stream. It shares the core idea of collision resolution by deeper branching, but differs from canonical HAMTs by using an infinite, domain-separated PRF stream instead of a single fixed hash, and by using fixed 16-way nodes without bitmap/path compression. The implementation is mutable; a persistent variant would require structural sharing.\n",
    "\n",
    "A **RadixHashChainMap** is a deterministic map structure that uses a radix trie keyed by **4-bit nibbles** emitted from a [`HashChainStream`](#hashchainstream).\n",
    "\n",
    "- Traversal begins at the root **`_Node`** (depth 0).\n",
    "- Each nibble from the hash-chain code indexes into the node’s fixed-fanout\n",
    "  `.child` array (16 slots).\n",
    "\n",
    "### Traversal Rules\n",
    "- **Leaf reached (`_Leaf`)** → a stored `(key, value)` binding has been found.\n",
    "- **Empty slot (`None`)** → no entry exists at this path; a new leaf can be inserted.\n",
    "- **Collision (two keys share a prefix)** →  \n",
    "  the existing leaf is replaced by a new internal node.  \n",
    "  Both the old and new entries are then re-inserted at the next nibble depth, \n",
    "  continuing until their paths diverge.\n",
    "\n",
    "### Key Properties\n",
    "- **Deterministic placement**: identical keys always traverse the same path.\n",
    "- **Prefix stability**: once placed, earlier nibbles never change.\n",
    "- **Collision resolution by depth**: ensures unique placement without overwriting.\n",
    "- **Compact structure**: internal nodes exist only where needed to separate colliding keys.\n",
    "\n",
    "---\n",
    "\n",
    "### Comparison: RadixHashChainMap vs Python `dict`\n",
    "\n",
    "| Aspect                  | `dict` (Hash Table)                                | `RadixHashChainMap` (Hash-Chain Trie)                  |\n",
    "|--------------------------|----------------------------------------------------|--------------------------------------------------------|\n",
    "| **Structure**            | Flat hash table with buckets and resizing logic    | Hierarchical trie of 4-bit nibbles                     |\n",
    "| **Key placement**        | Depends on hash modulo table size                  | Deterministic nibble-by-nibble path from HashChainStream |\n",
    "| **Collisions**           | Resolved via probing / chaining                    | Resolved by extending deeper into the trie             |\n",
    "| **Prefix stability**     | No (rehashing/resizing may relocate entries)       | Yes (prefix of path never changes)                     |\n",
    "| **Iteration order**      | Preserves insertion order (since Python 3.7)       | Arbitrary (depends on child traversal)                 |\n",
    "| **Memory usage**         | Compact, tuned for general-purpose use             | Potentially larger, but sparse and prunable            |\n",
    "| **Best use case**        | General-purpose fast lookups                       | Deterministic, hierarchical placement (e.g. CAS, tries) |\n",
    "\n",
    "---\n",
    "\n",
    "### How it’s like a HAMT\n",
    "* **Trie over hash chunks:** Index into the trie by fixed-width chunks of a hash (nibbles here), just like HAMT indexes by bit groups.\n",
    "* Collision handling by going deeper: When two keys share a prefix, descend further until a chunk differs.\n",
    "\n",
    "### Where it differs from a canonical HAMT\n",
    "\n",
    "**Hash source:**\n",
    "* **HAMT:** consumes slices of a single fixed hash of the key.\n",
    "* **HCA:** consumes an infinite PRF/hash-chain stream PRF(key || counter64_be(i)). That provides:\n",
    " * **True prefix stability** and **unbounded depth** without rehashing buckets.\n",
    " * Easy **domain separation** (personalization / keyed mode).\n",
    "\n",
    "**Branching & representation:**\n",
    "* **HAMT:** often uses bitmap-compressed nodes (e.g., 32-way logical fanout with sparse physical arrays) and sometimes path compression.\n",
    "* **HCA:** fixed 16-way arrays at every node; no bitmap or path compression (simpler, but heavier on memory).\n",
    "\n",
    "**Chunk width:**\n",
    "* **HAMT:** commonly 5 bits (32-way) for cache/perf tradeoffs.\n",
    "* **Yours:** 4 bits (16-way) MSB-first nibbles.\n",
    "\n",
    "**Mutability / persistence:**\n",
    "* **HAMT** (in literature): usually persistent/immutable with structural sharing (e.g., Clojure).\n",
    "* **HCA:** mutable (updates in place, explicit pruning).\n",
    "\n",
    "**Collision endgame:**\n",
    "* **HAMT:** with fixed hash length, true hash collisions may end in small buckets.\n",
    "* **HCA:** can always go deeper in the stream; no need for secondary buckets.\n",
    "\n",
    "### Possible RadixHashChainMap Updates\n",
    "\n",
    "* **Immutable/persistent API with structural sharing**\n",
    " 1) Walk down using nibbles from HashChainStream(key)\n",
    " 2) Copy each node on the path (shallow copy of 16-slot child array)\n",
    " 3) Modify the child pointer in the copied node\n",
    " 4) Return a new RadixHashChainMap with the new root; old m is untouched\n",
    "\n",
    "* **Add bitmap-compressed children (sparse arrays)**\n",
    "\n",
    "* **path compression for runs of single-child nodes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff927774-373f-4f4c-a7f7-1241cece4cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib, struct\n",
    "from typing import Optional, Iterator, Tuple, Any, List\n",
    "\n",
    "# ---------- RAM Radix (Nibble-Trie) with one key per anchor ----------\n",
    "\n",
    "class RadixHashChainMap:\n",
    "    \"\"\"\n",
    "    A persistent map structure backed by a radix trie over hash-chain nibbles.\n",
    "\n",
    "    Unlike a Python ``dict`` (which uses a hash table), this map organizes\n",
    "    keys by traversing the nibble sequence emitted from a\n",
    "    :class:`HashChainStream`. Each key’s path is deterministic and\n",
    "    **prefix-stable**: once a key is placed, its location in the trie\n",
    "    is fixed by its hash-chain nibble prefix and only extended if\n",
    "    collisions require deeper separation.\n",
    "\n",
    "    Core Features\n",
    "    -------------\n",
    "    - **Deterministic placement**: keys are mapped to a sequence of\n",
    "      4-bit nibbles derived from a PRF over (``key_bytes || counter``).\n",
    "    - **Prefix stability**: once emitted, earlier nibbles never change;\n",
    "      existing entries are unaffected by inserting new keys.\n",
    "    - **Collision resolution**: if two keys share the same prefix,\n",
    "      internal nodes are introduced until their paths diverge.\n",
    "    - **Pruning**: deletions clean up empty internal nodes so the trie\n",
    "      does not accumulate dead branches.\n",
    "    - **Dictionary-like API**: supports insertion, lookup, deletion,\n",
    "      and iteration via familiar methods.\n",
    "\n",
    "    Dictionary Interface\n",
    "    --------------------\n",
    "    - ``map[key] = value`` → insert or update a binding.\n",
    "    - ``map[key]`` → retrieve a value or raise ``KeyError``.\n",
    "    - ``del map[key]`` → remove a binding or raise ``KeyError``.\n",
    "    - ``key in map`` → membership test (calls ``__getitem__`` under the hood).\n",
    "    - ``len(map)`` → number of stored bindings.\n",
    "\n",
    "    Iteration Helpers\n",
    "    -----------------\n",
    "    - :meth:`items()` → yield ``(key, value)`` pairs.\n",
    "    - :meth:`keys()` → yield keys only.\n",
    "    - :meth:`values()` → yield values only.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Keys may be of any type supported by :class:`HashChainStream`\n",
    "      (str, bytes, or custom via ``encode_key``).\n",
    "    - The iteration order of keys/values/items is **not guaranteed**;\n",
    "      collect and sort explicitly if stable ordering is required.\n",
    "    - Because structure is based on nibble paths, the trie is well-suited\n",
    "      for applications that benefit from hierarchical, deterministic\n",
    "      placement rather than hash-table randomness.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> m = RadixHashChainMap()\n",
    "    >>> m[\"alpha\"] = 1\n",
    "    >>> m[\"beta\"] = 2\n",
    "    >>> len(m)\n",
    "    2\n",
    "    >>> \"alpha\" in m\n",
    "    True\n",
    "    >>> m[\"alpha\"]\n",
    "    1\n",
    "    >>> list(m.keys())\n",
    "    ['alpha', 'beta']\n",
    "    >>> del m[\"alpha\"]\n",
    "    >>> \"alpha\" in m\n",
    "    False\n",
    "    \"\"\"\n",
    "\n",
    "    # ----- node/leaf types -----\n",
    "\n",
    "    class _Node:\n",
    "        \"\"\"\n",
    "        16-way internal node in the `RadixHashChainMap` trie.\n",
    "    \n",
    "        Each `_Node` corresponds to a depth in the hash-chain stream,\n",
    "        where `depth` identifies which nibble index is being branched on.\n",
    "        Children are indexed by the value of that nibble (`0..15`).\n",
    "    \n",
    "        Attributes\n",
    "        ----------\n",
    "        depth : int\n",
    "            The global nibble index at which this node resides. For example,\n",
    "            `depth=0` partitions the map on the very first nibble from the\n",
    "            hash-chain stream, `depth=1` on the second nibble, etc.\n",
    "        child : list[Optional[RadixHashChainMap._Entry]]\n",
    "            Fixed fan-out array of size 16. Each element is either:\n",
    "            - `None` (no entry at this nibble path yet), or\n",
    "            - a reference to another `_Node` or `_Leaf`/`_Entry` that continues\n",
    "              the path deeper into the trie.\n",
    "    \n",
    "        Notes\n",
    "        -----\n",
    "        - The radix (fan-out) is always 16, matching the 4-bit nibble alphabet.\n",
    "        - This structure provides **O(depth)** lookup and insertion, where\n",
    "          `depth` is the number of nibbles consumed from the hash-chain stream.\n",
    "        - `_Node` is an internal building block; end-users interact with\n",
    "          `RadixHashChainMap` at a higher level.\n",
    "        \"\"\"\n",
    "\n",
    "        # Only create instance variables for the following attributes:\n",
    "        __slots__ = (\"depth\", \"child\")\n",
    "        \n",
    "        def __init__(self, depth: int):\n",
    "            \"\"\"\n",
    "            Initialize a 16-way branching node at the given nibble depth.\n",
    "        \n",
    "            Parameters\n",
    "            ----------\n",
    "            depth : int\n",
    "                The global nibble index this node represents.  \n",
    "                - `depth = 0` → branching on the very first nibble of the hash code.  \n",
    "                - `depth = 1` → branching on the second nibble.  \n",
    "                - …and so on.\n",
    "        \n",
    "            Attributes\n",
    "            ----------\n",
    "            depth : int\n",
    "                Stored depth (nibble index) for this node.\n",
    "            child : list[Optional[RadixHashChainMap._Entry]]\n",
    "                Fixed fan-out array of length 16. Each slot corresponds to one nibble value (0–15).\n",
    "                Each element may be:\n",
    "                  • `None` (no entry at that path yet),  \n",
    "                  • another `_Node` (continuing the trie), or  \n",
    "                  • a `_Leaf` entry storing a key/value binding.\n",
    "            \"\"\"\n",
    "            self.depth: int = depth\n",
    "            self.child: List[Optional[\"RadixHashChainMap._Entry\"]] = [None] * 16  # fixed fanout\n",
    "\n",
    "        class _Leaf:\n",
    "            \"\"\"\n",
    "            Leaf entry in the `RadixHashChainMap`.\n",
    "        \n",
    "            Stores the actual (key, value) binding for a completed path through the trie.\n",
    "        \n",
    "            Attributes\n",
    "            ----------\n",
    "            key : Any\n",
    "                The original key object (or its canonicalized form).\n",
    "            value : Any\n",
    "                The associated value stored in the map.\n",
    "            hash_hex : str\n",
    "                The hex string emitted from `HashChainStream.take_hex()`\n",
    "                that led to this leaf. Used to confirm placement and\n",
    "                resolve hash collisions by extending the path.\n",
    "            \"\"\"\n",
    "            __slots__ = (\"depth\", \"key\", \"value\")\n",
    "            \n",
    "            def __init__(self, depth: int, key: Any, value: Any):\n",
    "                \"\"\"\n",
    "                Initialize a leaf entry at the given nibble depth.\n",
    "            \n",
    "                Parameters\n",
    "                ----------\n",
    "                depth : int\n",
    "                    The global nibble index where this leaf resides. Indicates how many\n",
    "                    nibbles of the hash code were consumed to reach this binding.\n",
    "                key : Any\n",
    "                    The original key object (or its canonicalized form).\n",
    "                value : Any\n",
    "                    The associated value stored in the map.\n",
    "            \n",
    "                Attributes\n",
    "                ----------\n",
    "                depth : int\n",
    "                    Stored nibble index for this leaf.\n",
    "                key : Any\n",
    "                    Key object associated with this entry.\n",
    "                value : Any\n",
    "                    Value object bound to the key.\n",
    "                \"\"\"\n",
    "                self.depth: int = depth\n",
    "                self.key: Any = key\n",
    "                self.value: Any = value\n",
    "    \n",
    "        # A union type for children\n",
    "        _Entry = _Node | _Leaf\n",
    "    \n",
    "        # ----- map core -----\n",
    "    \n",
    "        def __init__(self):\n",
    "            \"\"\"\n",
    "            Initialize an empty `RadixHashChainMap`.\n",
    "        \n",
    "            Creates a fresh trie with a single root `_Node` at depth 0\n",
    "            (branching on the very first nibble of any hash code).  \n",
    "            The map is initially empty with size zero.\n",
    "        \n",
    "            Attributes\n",
    "            ----------\n",
    "            _root : RadixHashChainMap._Node\n",
    "                The root node of the radix trie, always at `depth = 0`.\n",
    "            _size : int\n",
    "                Number of key/value entries currently stored in the map.\n",
    "            \"\"\"\n",
    "            self._root: RadixHashChainMap._Node = RadixHashChainMap._Node(depth=0)\n",
    "            self._size: int = 0\n",
    "    \n",
    "        def __len__(self) -> int:\n",
    "            \"\"\"\n",
    "            Return the number of entries stored in the map.\n",
    "        \n",
    "            Returns\n",
    "            -------\n",
    "            int\n",
    "                The count of key/value bindings currently present in the\n",
    "                `RadixHashChainMap`.\n",
    "        \n",
    "            Examples\n",
    "            --------\n",
    "            >>> m = RadixHashChainMap()\n",
    "            >>> len(m)\n",
    "            0\n",
    "            >>> m.insert(\"a\", 1)\n",
    "            >>> len(m)\n",
    "            1\n",
    "            \"\"\"\n",
    "            return self._size\n",
    "    \n",
    "        def __contains__(self, key: Any) -> bool:\n",
    "            \"\"\"\n",
    "            Return True if the map contains an entry for `key`.\n",
    "        \n",
    "            This allows use of the ``in`` operator, e.g.\n",
    "            ``if k in map: ...``. Internally it delegates to\n",
    "            ``__getitem__`` and catches a ``KeyError``.\n",
    "        \n",
    "            Parameters\n",
    "            ----------\n",
    "            key : Any\n",
    "                The lookup key. It will be canonicalized and\n",
    "                resolved through the radix trie.\n",
    "        \n",
    "            Returns\n",
    "            -------\n",
    "            bool\n",
    "                True if the key exists in the map, False otherwise.\n",
    "        \n",
    "            Examples\n",
    "            --------\n",
    "            >>> m = RadixHashChainMap()\n",
    "            >>> m.insert(\"a\", 1)\n",
    "            >>> \"a\" in m\n",
    "            True\n",
    "            >>> \"b\" in m\n",
    "            False\n",
    "            \"\"\"\n",
    "            try:\n",
    "                _ = self[key]\n",
    "                return True\n",
    "            except KeyError:\n",
    "                return False\n",
    "    \n",
    "        # ----- public API -----\n",
    "    \n",
    "        def __setitem__(self, key: Any, value: Any) -> None:\n",
    "            \"\"\"\n",
    "            Insert or update a `key -> value` binding in the map.\n",
    "        \n",
    "            This implements the assignment operator (`map[key] = value`).\n",
    "            Insertion paths are guided by the nibble sequence emitted from\n",
    "            `HashChainStream(key)`:\n",
    "        \n",
    "            - **Empty slot**: if the first differing nibble position leads to\n",
    "              an unused child slot, a new leaf is created.\n",
    "            - **Same key**: if the path ends at a leaf with the same key,\n",
    "              its value is simply overwritten.\n",
    "            - **Collision**: if the path ends at a leaf with a *different* key,\n",
    "              an internal node is introduced at the current depth, and both\n",
    "              keys are pushed deeper until their next nibble diverges. At that\n",
    "              divergence, both leaves are placed.\n",
    "            - **Existing node**: if the child is already an internal node,\n",
    "              the algorithm descends and continues the process.\n",
    "        \n",
    "            Parameters\n",
    "            ----------\n",
    "            key : Any\n",
    "                The lookup key. It is canonicalized through `HashChainStream`\n",
    "                to produce a deterministic sequence of nibbles.\n",
    "            value : Any\n",
    "                The value to associate with the key.\n",
    "        \n",
    "            Notes\n",
    "            -----\n",
    "            - This design ensures prefix stability: once placed, a leaf’s\n",
    "              position is determined by the prefix of its hash-chain nibble\n",
    "              path, and only extended if collisions require deeper separation.\n",
    "            - `__setitem__` increments the map size only on a *new* insertion,\n",
    "              not when updating an existing key.\n",
    "        \n",
    "            Examples\n",
    "            --------\n",
    "            >>> m = RadixHashChainMap()\n",
    "            >>> m[\"a\"] = 1   # insert\n",
    "            >>> m[\"a\"] = 2   # update\n",
    "            >>> m[\"b\"] = 3   # insert with different key\n",
    "            >>> len(m)\n",
    "            2\n",
    "            \"\"\"\n",
    "            h_new = HashChainStream(key)\n",
    "            node = self._root\n",
    "            d = node.depth  # 0\n",
    "    \n",
    "            while True:\n",
    "                nib = h_new.nibble(d)\n",
    "                child = node.child[nib]\n",
    "    \n",
    "                if child is None:\n",
    "                    # Empty slot: place a new leaf at depth d+1\n",
    "                    node.child[nib] = self._Leaf(depth=d+1, key=key, value=value)\n",
    "                    self._size += 1\n",
    "                    return\n",
    "    \n",
    "                if isinstance(child, self._Leaf):\n",
    "                    if child.key == key:\n",
    "                        # Update in place\n",
    "                        child.value = value\n",
    "                        return\n",
    "    \n",
    "                    # Collision with a *different* key: separate by going deeper\n",
    "                    # Create a new internal node at depth d+1 replacing the leaf slot.\n",
    "                    new_node = self._Node(depth=d+1)\n",
    "                    node.child[nib] = new_node\n",
    "    \n",
    "                    # We'll place both the existing leaf and the new key below new_node,\n",
    "                    # possibly creating more levels if their next nibbles keep matching.\n",
    "                    h_old = HashChainStream(child.key)\n",
    "                    depth = new_node.depth  # start at d+1\n",
    "    \n",
    "                    while True:\n",
    "                        nib_old = h_old.nibble(depth)\n",
    "                        nib_new = h_new.nibble(depth)\n",
    "    \n",
    "                        if nib_old != nib_new:\n",
    "                            # Divergence point: attach two leaves at depth+1\n",
    "                            new_node.child[nib_old] = self._Leaf(depth=depth+1, key=child.key, value=child.value)\n",
    "                            new_node.child[nib_new] = self._Leaf(depth=depth+1, key=key, value=value)\n",
    "                            self._size += 1\n",
    "                            return\n",
    "    \n",
    "                        # Still colliding: create another internal node and descend\n",
    "                        next_node = new_node.child[nib_new]\n",
    "                        if next_node is None:\n",
    "                            next_node = self._Node(depth=depth+1)\n",
    "                            new_node.child[nib_new] = next_node\n",
    "                        new_node = next_node\n",
    "                        depth = new_node.depth\n",
    "    \n",
    "                else:\n",
    "                    # Child is an internal node: descend and continue\n",
    "                    node = child\n",
    "                    d = node.depth\n",
    "    \n",
    "        def __getitem__(self, key: Any) -> Any:\n",
    "            \"\"\"\n",
    "            Retrieve the value associated with `key`.\n",
    "        \n",
    "            This implements the lookup operator (`value = map[key]`).  \n",
    "            The lookup proceeds nibble-by-nibble along the path emitted from\n",
    "            `HashChainStream(key)`:\n",
    "        \n",
    "            - **Empty slot (`None`)** → the key is not present, raise `KeyError`.\n",
    "            - **Leaf node** → if the leaf’s stored key matches, return its value;\n",
    "              otherwise, the path diverged only at deeper nibbles, so raise `KeyError`.\n",
    "            - **Internal node** → descend and continue until a leaf or empty slot is reached.\n",
    "        \n",
    "            Parameters\n",
    "            ----------\n",
    "            key : Any\n",
    "                The key to look up. Canonicalized via `HashChainStream` to\n",
    "                generate the deterministic nibble path.\n",
    "        \n",
    "            Returns\n",
    "            -------\n",
    "            Any\n",
    "                The value bound to `key`.\n",
    "        \n",
    "            Raises\n",
    "            ------\n",
    "            KeyError\n",
    "                If `key` is not present in the map.\n",
    "        \n",
    "            Notes\n",
    "            -----\n",
    "            - Lookup mirrors the insertion logic of `__setitem__`.\n",
    "            - Prefix stability ensures that once a key is placed, its path is\n",
    "              deterministic and retrievable without ambiguity.\n",
    "        \n",
    "            Examples\n",
    "            --------\n",
    "            >>> m = RadixHashChainMap()\n",
    "            >>> m[\"x\"] = 42\n",
    "            >>> m[\"x\"]\n",
    "            42\n",
    "            >>> m[\"y\"]\n",
    "            Traceback (most recent call last):\n",
    "                ...\n",
    "            KeyError: 'y'\n",
    "            \"\"\"\n",
    "            h = HashChainStream(key)\n",
    "            node = self._root\n",
    "            d = node.depth\n",
    "    \n",
    "            while True:\n",
    "                nib = h.nibble(d)\n",
    "                child = node.child[nib]\n",
    "                if child is None:\n",
    "                    raise KeyError(key)\n",
    "                if isinstance(child, self._Leaf):\n",
    "                    if child.key == key:\n",
    "                        return child.value\n",
    "                    # Same prefix up to child.depth-1 but different key → not present.\n",
    "                    raise KeyError(key)\n",
    "                node = child\n",
    "                d = node.depth\n",
    "    \n",
    "        def __delitem__(self, key: Any) -> None:\n",
    "            \"\"\"\n",
    "            Remove the binding for `key`.\n",
    "        \n",
    "            This implements the deletion operator (`del map[key]`).\n",
    "            The algorithm follows the nibble path generated by\n",
    "            `HashChainStream(key)` until it reaches a leaf:\n",
    "        \n",
    "            - **Empty slot (`None`)** → the key is not present, raise `KeyError`.\n",
    "            - **Leaf node** → if the leaf’s stored key matches, remove it.\n",
    "              Otherwise, the key is not present, raise `KeyError`.\n",
    "            - **Internal node** → descend until a leaf or empty slot is reached.\n",
    "        \n",
    "            After removal, the method **prunes** any internal nodes that\n",
    "            became empty as a result of the deletion. This ensures the\n",
    "            trie does not accumulate dead branches.\n",
    "        \n",
    "            Parameters\n",
    "            ----------\n",
    "            key : Any\n",
    "                The key to remove. Canonicalized via `HashChainStream` to\n",
    "                determine the nibble path.\n",
    "        \n",
    "            Raises\n",
    "            ------\n",
    "            KeyError\n",
    "                If `key` is not present in the map.\n",
    "        \n",
    "            Notes\n",
    "            -----\n",
    "            - Decrementing `self._size` happens only upon successful removal.\n",
    "            - The root node is never pruned, even if the map becomes empty.\n",
    "        \n",
    "            Examples\n",
    "            --------\n",
    "            >>> m = RadixHashChainMap()\n",
    "            >>> m[\"a\"] = 1\n",
    "            >>> m[\"b\"] = 2\n",
    "            >>> del m[\"a\"]\n",
    "            >>> \"a\" in m\n",
    "            False\n",
    "            >>> del m[\"z\"]\n",
    "            Traceback (most recent call last):\n",
    "                ...\n",
    "            KeyError: 'z'\n",
    "            \"\"\"\n",
    "            # Stack of (node, nib) so we can prune upward\n",
    "            stack: List[tuple[RadixHashChainMap._Node, int]] = []\n",
    "            h = HashChainStream(key)\n",
    "            node = self._root\n",
    "            d = node.depth\n",
    "    \n",
    "            while True:\n",
    "                nib = h.nibble(d)\n",
    "                stack.append((node, nib))\n",
    "                child = node.child[nib]\n",
    "                if child is None:\n",
    "                    raise KeyError(key)\n",
    "                if isinstance(child, self._Leaf):\n",
    "                    if child.key != key:\n",
    "                        raise KeyError(key)\n",
    "                    # Remove the leaf\n",
    "                    node.child[nib] = None\n",
    "                    self._size -= 1\n",
    "                    break\n",
    "                node = child\n",
    "                d = node.depth\n",
    "    \n",
    "            # Prune empty internal nodes\n",
    "            while stack:\n",
    "                parent, nib = stack.pop()\n",
    "                # If the child we just removed made this parent empty, and parent is not root, prune it.\n",
    "                if parent is self._root:\n",
    "                    break\n",
    "                if any(parent.child):\n",
    "                    break\n",
    "                # parent is empty: remove it from its own parent (peek)\n",
    "                if not stack:\n",
    "                    break\n",
    "                grand, g_nib = stack[-1]\n",
    "                grand.child[g_nib] = None\n",
    "    \n",
    "        # ----- optional iterators -----\n",
    "    \n",
    "        def items(self) -> Iterator[Tuple[Any, Any]]:\n",
    "            \"\"\"\n",
    "            Iterate over all `(key, value)` pairs stored in the map.\n",
    "        \n",
    "            Traverses the entire trie in depth-first order, yielding\n",
    "            each leaf’s stored binding. The order is **not guaranteed**:\n",
    "            it depends on the traversal of child arrays and may vary\n",
    "            between runs or Python versions. For deterministic ordering,\n",
    "            collect results into a list and sort explicitly.\n",
    "        \n",
    "            Yields\n",
    "            ------\n",
    "            Tuple[Any, Any]\n",
    "                The `(key, value)` pair for each leaf present in the map.\n",
    "        \n",
    "            Notes\n",
    "            -----\n",
    "            - This is the radix-map analogue of `dict.items()`.\n",
    "            - Internal nodes are traversal points only; only leaves\n",
    "              yield results.\n",
    "            - Safe to call on an empty map (yields nothing).\n",
    "        \n",
    "            Examples\n",
    "            --------\n",
    "            >>> m = RadixHashChainMap()\n",
    "            >>> m[\"x\"] = 1\n",
    "            >>> m[\"y\"] = 2\n",
    "            >>> sorted(m.items())\n",
    "            [('x', 1), ('y', 2)]\n",
    "            \"\"\"\n",
    "            stack: List[RadixHashChainMap._Entry] = [self._root]\n",
    "            while stack:\n",
    "                cur = stack.pop()\n",
    "                if isinstance(cur, self._Leaf):\n",
    "                    yield (cur.key, cur.value)\n",
    "                else:\n",
    "                    # push children\n",
    "                    for ch in cur.child:\n",
    "                        if ch is not None:\n",
    "                            stack.append(ch)\n",
    "    \n",
    "        def keys(self) -> Iterator[Any]:\n",
    "            \"\"\"\n",
    "            Iterate over all keys stored in the map.\n",
    "        \n",
    "            Traverses the trie and yields each leaf’s key.  \n",
    "            This is the radix-map analogue of `dict.keys()`.\n",
    "        \n",
    "            Yields\n",
    "            ------\n",
    "            Any\n",
    "                The key stored at each leaf.\n",
    "        \n",
    "            Notes\n",
    "            -----\n",
    "            - The order is not guaranteed; collect and sort\n",
    "              for deterministic results.\n",
    "            - Safe to call on an empty map (yields nothing).\n",
    "        \n",
    "            Examples\n",
    "            --------\n",
    "            >>> m = RadixHashChainMap()\n",
    "            >>> m[\"alpha\"] = 1\n",
    "            >>> m[\"beta\"] = 2\n",
    "            >>> list(m.keys())\n",
    "            ['alpha', 'beta']\n",
    "            \"\"\"\n",
    "            for k, _v in self.items():\n",
    "                yield k\n",
    "    \n",
    "        def values(self) -> Iterator[Any]:\n",
    "            \"\"\"\n",
    "            Iterate over all values stored in the map.\n",
    "        \n",
    "            Traverses the trie and yields each leaf’s value.  \n",
    "            This is the radix-map analogue of `dict.values()`.\n",
    "        \n",
    "            Yields\n",
    "            ------\n",
    "            Any\n",
    "                The value stored at each leaf.\n",
    "        \n",
    "            Notes\n",
    "            -----\n",
    "            - The order is not guaranteed; collect and sort\n",
    "              alongside keys if deterministic pairing is needed.\n",
    "            - Safe to call on an empty map (yields nothing).\n",
    "        \n",
    "            Examples\n",
    "            --------\n",
    "            >>> m = RadixHashChainMap()\n",
    "            >>> m[\"foo\"] = 10\n",
    "            >>> m[\"bar\"] = 20\n",
    "            >>> list(m.values())\n",
    "            [10, 20]\n",
    "            \"\"\"\n",
    "            for _k, v in self.items():\n",
    "                yield v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b543a9c-31ab-45e2-ae70-830b5e6b74ea",
   "metadata": {},
   "source": [
    "### RadixHashChainMap Examples\n",
    "\n",
    "The following examples demonstrate how to use a `RadixHashChainMap` much like a Python `dict`, \n",
    "while benefiting from its deterministic, nibble-by-nibble trie structure.\n",
    "\n",
    "#### Basic Insertion & Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d615ff4b-423c-4d3c-acbf-8a477de9e47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "m = RadixHashChainMap()\n",
    "m[\"alpha\"] = 1\n",
    "m[\"beta\"] = 2\n",
    "\n",
    "print(len(m))         # → 2\n",
    "print(m[\"alpha\"])     # → 1\n",
    "print(\"beta\" in m)    # → True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300a5b4f-70f0-42e7-99be-6ee524951bdd",
   "metadata": {},
   "source": [
    "#### Updating an Existing Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14e0c0e2-0433-4bf3-9b24-3738fdd9fc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "m[\"alpha\"] = 42\n",
    "print(m[\"alpha\"])     # → 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9530770e-379e-4cb4-85c4-81ad07e8c4a8",
   "metadata": {},
   "source": [
    "#### Deletion with Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb47ab5e-f418-4acb-82b2-7a110570433e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "del m[\"beta\"]\n",
    "print(\"beta\" in m)    # → False\n",
    "print(len(m))         # → 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efb6ea6-98a1-4742-b92f-87f4ee45ab6f",
   "metadata": {},
   "source": [
    "#### Iterating Keys, Values, Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e081194-541e-4a07-bd70-e89c6ef05e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['delta', 'gamma', 'alpha']\n",
      "[4, 3, 42]\n",
      "[('delta', 4), ('gamma', 3), ('alpha', 42)]\n"
     ]
    }
   ],
   "source": [
    "m[\"gamma\"] = 3\n",
    "m[\"delta\"] = 4\n",
    "\n",
    "print(list(m.keys()))      # → ['alpha', 'gamma', 'delta']  (order not guaranteed)\n",
    "print(list(m.values()))    # → [42, 3, 4]\n",
    "print(list(m.items()))     # → [('alpha', 42), ('gamma', 3), ('delta', 4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcda53e-f30b-4861-b82a-8fd3a1807db7",
   "metadata": {},
   "source": [
    "#### Deterministic Paths\n",
    "\n",
    "Because each key’s path is determined by its hash-chain nibble sequence,\n",
    "prefix stability is guaranteed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec4b8e3d-66b1-4eba-9515-cc31f4ad8bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "afc31693714b7c1c\n"
     ]
    }
   ],
   "source": [
    "m = RadixHashChainMap()\n",
    "m[\"x\"] = 100\n",
    "hx = HashChainStream(\"x\").take_hex(16)\n",
    "print(hx) \n",
    "# This hex string represents the deterministic nibble path for 'x'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00e4320-7643-43a8-b8db-df926887c549",
   "metadata": {},
   "source": [
    "#### Collision Handling\n",
    "\n",
    "Keys with long common prefixes in their hash-chain output will share internal nodes\n",
    "until their nibble sequences diverge. Collisions are resolved automatically by\n",
    "extending the trie just far enough to separate the keys.\n",
    "\n",
    "---\n",
    "\n",
    "#### Visual Walk-through of Trie Traversal\n",
    "\n",
    "Suppose we insert two keys `\"cat\"` and `\"car\"`. Their hash-chain nibbles may start like this:\n",
    "\n",
    "- `HashChainStream(\"cat\").take_hex(4) → a3f1`\n",
    "- `HashChainStream(\"car\").take_hex(4) → a3d9`\n",
    "\n",
    "Both share the prefix `a3` but diverge at the third nibble (`f` vs `d`).\n",
    "\n",
    "**Trie structure after insertion:**\n",
    "```text\n",
    "(root)\n",
    " └── nibble 'a'\n",
    "      └── nibble '3'\n",
    "           ├── nibble 'f' → Leaf(\"cat\", value)\n",
    "           └── nibble 'd' → Leaf(\"car\", value)\n",
    "```\n",
    "\n",
    "* Traversal consumes one nibble at a time from the hash-chain.\n",
    "* Internal _Nodes are created automatically at each depth when paths diverge.\n",
    "* The first differing nibble ensures unique placement without overwriting.\n",
    "\n",
    "**This illustrates collision resolution by depth:**\n",
    "* the trie extends only as far as needed to separate colliding keys, while\n",
    "* prefixes remain stable for future lookups.\n",
    "\n",
    "---\n",
    "\n",
    "### Programmatic Collision Demo (with real keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80dd0adf-fd00-43da-b279-ae9486b71f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found collision on first 3 nibbles (algo=blake2b):\n",
      "'demo-14' → cf78284e  (nibbles: c f 7 8 2 8 4 e)\n",
      "'demo-48' → cf77adde  (nibbles: c f 7 7 a d d e)\n",
      "Shared prefix (first 3 nibbles): cf7\n",
      "First divergence at nibble index d = 3\n",
      "Lookup:\n",
      "demo-14 → value1\n",
      "demo-48 → value2\n",
      "\n",
      "Local trie sketch (conceptual):\n",
      "(root)\n",
      " └── … shared 3 nibbles 'cf7'\n",
      "      ├── nibble for 'demo-14' at d=3 → Leaf('demo-14', 'value1')\n",
      "      └── nibble for 'demo-48' at d=3 → Leaf('demo-48', 'value2')\n"
     ]
    }
   ],
   "source": [
    "# --- Collision demo utilities ---\n",
    "\n",
    "from typing import Tuple, Optional, Dict\n",
    "from itertools import count\n",
    "\n",
    "# Adjust imports to your module layout\n",
    "# from your_module import HashChainStream, RadixHashChainMap\n",
    "\n",
    "def nibble_prefix_hex(key, L: int, *, algo=\"blake2b\"):\n",
    "    \"\"\"Return the first L nibbles of the key's stream as hex.\"\"\"\n",
    "    return HashChainStream(key, algo=algo).take_hex(L)\n",
    "\n",
    "def find_prefix_collision(L: int, *, algo=\"blake2b\", seed=\"k\", limit=200_000) -> Tuple[str, str, str]:\n",
    "    \"\"\"\n",
    "    Find two distinct strings seed+N that share the same first L nibbles\n",
    "    under the chosen PRF. Returns (k1, k2, shared_prefix).\n",
    "    \"\"\"\n",
    "    seen: Dict[str, str] = {}\n",
    "    for n in count():\n",
    "        k = f\"{seed}{n}\"\n",
    "        pref = nibble_prefix_hex(k, L, algo=algo)\n",
    "        if pref in seen and seen[pref] != k:\n",
    "            return seen[pref], k, pref\n",
    "        seen[pref] = seen.get(pref, k)\n",
    "        if n >= limit:\n",
    "            raise RuntimeError(f\"No collision found for L={L} within {limit} tries (algo={algo})\")\n",
    "\n",
    "def first_divergence(k1, k2, *, algo=\"blake2b\", max_nibbles=128) -> int:\n",
    "    \"\"\"\n",
    "    Return the nibble index at which the two keys first differ.\n",
    "    If identical up to max_nibbles, returns max_nibbles.\n",
    "    \"\"\"\n",
    "    h1 = HashChainStream(k1, algo=algo)\n",
    "    h2 = HashChainStream(k2, algo=algo)\n",
    "    for d in range(max_nibbles):\n",
    "        if h1.nibble(d) != h2.nibble(d):\n",
    "            return d\n",
    "    return max_nibbles\n",
    "\n",
    "def show_path(key, n=16, *, algo=\"blake2b\"):\n",
    "    \"\"\"Pretty-print the first n nibbles of a key's path.\"\"\"\n",
    "    hx = HashChainStream(key, algo=algo).take_hex(n)\n",
    "    print(f\"{key!r} → {hx}  (nibbles: {' '.join(hx)})\")\n",
    "\n",
    "# --- Find an actual collision and insert into the map ---\n",
    "\n",
    "L = 3                 # number of shared prefix nibbles to require (tweak 2..4)\n",
    "ALGO = \"blake2b\"      # or \"blake3\", \"xxh3\", \"xxh64\"\n",
    "\n",
    "k1, k2, pref = find_prefix_collision(L, algo=ALGO, seed=\"demo-\")\n",
    "print(f\"Found collision on first {L} nibbles (algo={ALGO}):\")\n",
    "show_path(k1, n=8, algo=ALGO)\n",
    "show_path(k2, n=8, algo=ALGO)\n",
    "print(f\"Shared prefix (first {L} nibbles): {pref}\")\n",
    "\n",
    "div = first_divergence(k1, k2, algo=ALGO, max_nibbles=64)\n",
    "print(f\"First divergence at nibble index d = {div}\")\n",
    "\n",
    "# Build the map and insert both keys\n",
    "m = RadixHashChainMap()\n",
    "m[k1] = \"value1\"\n",
    "m[k2] = \"value2\"\n",
    "\n",
    "print(\"Lookup:\")\n",
    "print(k1, \"→\", m[k1])\n",
    "print(k2, \"→\", m[k2])\n",
    "\n",
    "# Optional: tiny ASCII sketch showing the structure around the divergence\n",
    "print(\"\\nLocal trie sketch (conceptual):\")\n",
    "print(\"(root)\")\n",
    "print(f\" └── … shared {L} nibbles '{pref}'\")\n",
    "print(f\"      ├── nibble for {k1!r} at d={div} → Leaf({k1!r}, 'value1')\")\n",
    "print(f\"      └── nibble for {k2!r} at d={div} → Leaf({k2!r}, 'value2')\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e718862f-ebd0-4608-b923-d45719e93de4",
   "metadata": {},
   "source": [
    "### Explanation of observed outputs (example run above):\n",
    "\n",
    "* **cf78284e** vs **cf77adde** are the first 8 nibbles (hex digits) of the hash-chain paths for 'demo-14' and 'demo-48' (MSB-first).\n",
    "* Shared prefix: cf7 means the first 3 nibbles match (c, f, 7).\n",
    "* First divergence at d = 3 means the 4th nibble differs:\n",
    " * 'demo-14' has nibble 8 at d=3 (hex cf7**8**…)\n",
    " * 'demo-48' has nibble 7 at d=3 (hex cf7**7**…)\n",
    "\n",
    "**The ASCII sketch mirrors the trie around that divergence:**\n",
    "* After consuming the shared prefix c→f→7, the next nibble branches to two leaves:\n",
    "  * d=3 → 8 for 'demo-14'\n",
    "  * d=3 → 7 for 'demo-48'\n",
    "* **Lookups succeed** for both keys (value1, value2) because the trie stores both leaves under different branches after the divergence.\n",
    "\n",
    "**Tip:** Increase L to find longer shared prefixes (more pronounced collisions). If you switch ALGO, the actual keys found and the prefix values will differ, but the behavior of collision handling in the trie remains the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3767aa9-0d1e-4383-a420-6dccd2744175",
   "metadata": {},
   "source": [
    "### Recompute to Assert Behavior (adjust `ALGO` / keys as needed)\n",
    "\n",
    "Use the exact keys found by the collision demo to **verify** the shared prefix,\n",
    "the first divergence, and correct map behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b6f94f86-d966-467b-8094-6426ce7560b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nibble @ d=3: 8 7\n"
     ]
    }
   ],
   "source": [
    "ALGO = \"blake2b\"\n",
    "k1, k2 = \"demo-14\", \"demo-48\"\n",
    "h1 = HashChainStream(k1, algo=ALGO)\n",
    "h2 = HashChainStream(k2, algo=ALGO)\n",
    "\n",
    "# Assert shared first 3 nibbles, divergence at 3\n",
    "assert h1.take_hex(3) == h2.take_hex(3) == \"cf7\"\n",
    "for d in range(3):\n",
    "    assert h1.nibble(d) == h2.nibble(d)\n",
    "assert h1.nibble(3) != h2.nibble(3)\n",
    "\n",
    "# Map behavior\n",
    "m = RadixHashChainMap()\n",
    "m[k1] = \"value1\"\n",
    "m[k2] = \"value2\"\n",
    "assert m[k1] == \"value1\"\n",
    "assert m[k2] == \"value2\"\n",
    "assert len(m) == 2\n",
    "\n",
    "# Optional: show the exact diverging nibbles\n",
    "print(\"nibble @ d=3:\", h1.nibble(3), h2.nibble(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff383de-6fb2-4b0e-b52c-ea1259cedf6c",
   "metadata": {},
   "source": [
    "### Explanation of observed outputs (example run above):\n",
    "* **take_hex(3) == \"cf7\":** the first 3 nibbles (hex digits) match for both keys.\n",
    "* **nibble(3) differs:** the 4th nibble is different (8 vs 7), which is where\n",
    "the trie branches to two leaves.\n",
    "* **Both insertions succeed and len(m) == 2:** confirms that collision handling\n",
    "inserts both keys under the shared prefix and diverging nibble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15900b1-8898-4059-b782-783cb700fd36",
   "metadata": {},
   "source": [
    "### Inspecting Nibbles for Two Colliding Keys\n",
    "\n",
    "Before asserting behavior, it’s helpful to **print and eyeball** the first few\n",
    "nibbles (hex digits) for each key’s hash-chain path. Below, we:\n",
    "\n",
    "- Build two streams for the colliding keys (`\"demo-14\"` and `\"demo-48\"`).\n",
    "- Print the first 8 hex digits for each, along with a spaced version to see\n",
    "  the **MSB-first nibble sequence** clearly.\n",
    "- Verify that the first **3 nibbles** match (`\"cf7\"`) and that the **4th nibble**\n",
    "  (index `d = 3`) diverges: `8` for `\"demo-14\"`, `7` for `\"demo-48\"`.\n",
    "\n",
    "> Recall: nibble index `d` is 0-based and MSB-first:  \n",
    "> `d=0→1st hex`, `d=1→2nd hex`, `d=2→3rd hex`, `d=3→4th hex`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "39577477-a3bd-49b5-a601-6b1ca4ca7e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cf78284e  |  c f 7 8 2 8 4 e\n",
      "cf77adde  |  c f 7 7 a d d e\n"
     ]
    }
   ],
   "source": [
    "h1 = HashChainStream(\"demo-14\", algo=\"blake2b\")\n",
    "h2 = HashChainStream(\"demo-48\", algo=\"blake2b\")\n",
    "\n",
    "hx1 = h1.take_hex(8); hx2 = h2.take_hex(8)\n",
    "print(hx1, \" | \", \" \".join(hx1))\n",
    "print(hx2, \" | \", \" \".join(hx2))\n",
    "\n",
    "assert hx1[:3] == hx2[:3] == \"cf7\"\n",
    "assert h1.nibble(3) == 8 and h2.nibble(3) == 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9f996f-9fe6-40e8-b80e-e6ea89020d43",
   "metadata": {},
   "source": [
    "### Explanation of observed outputs (example run above):\n",
    "- **MSB-first nibbles:** Each hex digit is one nibble read MSB-first from a 64-bit block stream.\n",
    "- **Shared prefix:** The first **three** nibbles match — `c f 7` → shared prefix `\"cf7\"`.\n",
    "- **First divergence (d = 3):** The **4th** nibble differs:\n",
    "  - Stream 1: `8` → sequence `c f 7 **8** …`\n",
    "  - Stream 2: `7` → sequence `c f 7 **7** …`\n",
    "- **Trie effect:** In `RadixHashChainMap`, traversal follows `c → f → 7` together, then branches at depth `d = 3` into two children (`8` vs `7`), each leading to its own leaf.\n",
    "- **Determinism:** Given the same keys and PRF (`blake2b` here), these prefixes and the divergence point are deterministic and repeatable across runs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817bb43a-5841-4755-97a1-fc43ad269918",
   "metadata": {},
   "source": [
    "# HashChainAnchorDict Using HCA (Hash-Chain Augmentation)\n",
    "\n",
    "**HashChainAnchorDict (HAD)** is a hash table that anchors each key at insertion to a `(depth, prefix)` derived from a prefix-stable HashChainStream. It provides 1-hop lookups via the recorded anchor, local growth (no global rehash), and deterministic/self-healing addressing suitable for persistence and sharding.\n",
    "\n",
    "At insert time, HAD consumes `d` MSB-first 4-bit nibbles from the hash-chain stream of the key to form a packed prefix. The pair `(d, prefix)` is the **anchor id**. An anchor holds a tiny, cache-friendly microtable (or a single leaf) for keys that shared that creation prefix at the moment of insertion. If an anchor becomes \"hot,\" new keys targeting it simply choose a deeper `(d+1, prefix')`—existing keys never move.\n",
    "\n",
    "## Placement & Lookup\n",
    "\n",
    "**Anchor computation:** `prefix = pack(HCS.nibble(0..d-1))` → anchor id `(d, prefix)`.\n",
    "\n",
    "### Insert (local growth):\n",
    "\n",
    "1. Start with a target depth `d_target` (e.g., from table size).\n",
    "2. If the chosen anchor's microtable is under capacity, place the key there.\n",
    "3. Otherwise bump depth (`d ← d+1`) and retry. Only the new key goes deeper.\n",
    "\n",
    "### Get (1-hop): \n",
    "Use the recorded `(d, prefix)` for the key to jump directly to its anchor and resolve inside the microtable. (If `d` wasn't cached, a bounded search over nearby depths reconstructs it.)\n",
    "\n",
    "**Collision policy:** collisions never cause probe chains across the whole table or global rehashes; they are absorbed by creating a deeper anchor only for the colliding newcomer.\n",
    "\n",
    "## Key Properties\n",
    "\n",
    "- **Deterministic addressing:** `(d, prefix)` is a pure function of `(key, policy, HCS version)`. Paths do not change over time.\n",
    "- **No global resize:** growth is local (deepen hot anchors) rather than reindexing the entire table.\n",
    "- **1-hop lookups:** `(d, prefix)` → anchor → value (with a tiny per-anchor probe).\n",
    "- **Self-healing:** anchors can be rebuilt independently from the keys (and optional stored `insert_depth`) without scanning a monolithic table.\n",
    "- **Sharding by prefix:** anchors form natural partitions for distributed placement and repair.\n",
    "\n",
    "## Comparison: HashChainAnchorDict vs Python dict\n",
    "\n",
    "| Aspect | dict (Open Addressing) | HashChainAnchorDict (Anchors + HCS) |\n",
    "|--------|------------------------|-------------------------------------|\n",
    "| **Indexing** | `index = hash(key) & mask`; probe sequence with perturbation | Anchor id `(d, prefix)` from hash-chain nibbles |\n",
    "| **Collisions** | Resolved by probing within a single flat array | Resolved by local depth bump (deeper anchor); existing keys stay put |\n",
    "| **Resizes** | Global reindex when table grows | None; only hot anchors deepen |\n",
    "| **Determinism** | Low (salt, resizes change placement) | High; `(d, prefix)` is stable and reproducible |\n",
    "| **Lookup path** | ~1–3 contiguous probes | 1 hop to anchor + tiny microtable probe |\n",
    "| **Tail latency** | Spikes during resize / pathological clusters | Flat; no global operations |\n",
    "| **Persistence / rebuild** | Reinsert all keys into a new table | Rebuild per-anchor from keys; self-healing |\n",
    "| **Best use** | Small/medium in-RAM maps, general purpose | Large/persistent KV, sharding, verifiable storage, predictable SLOs |\n",
    "\n",
    "## Relation to Extendible Hashing & HAMT\n",
    "\n",
    "**Extendible Hashing:** similar \"local depth\" idea (directory bit-depth grows per hot bucket). HAD replaces the fixed hash with an extendable, prefix-stable stream, so it never needs a global directory rewrite and can deepen arbitrarily without rehash.\n",
    "\n",
    "**HAMT:** branches by fixed-width hash slices. HAD keeps a flat, cache-aware anchor layer instead of a full trie, using the hash-chain only to compute stable prefix anchors and local deeper anchors on demand.\n",
    "\n",
    "## Why HCA (Hash-Chain) matters here\n",
    "\n",
    "- **Prefix stability:** earlier nibbles never change, so an anchor's identity is permanent.\n",
    "- **Unbounded entropy:** always take more nibbles to split a hot anchor—no fixed 64/128-bit ceiling.\n",
    "- **Domain separation:** the HCS person/seed defines namespaces; evolution or multi-tenant layouts don't collide.\n",
    "- **Self-healing guarantees:** given keys (and optionally `insert_depth`), anchor membership is re-derivable ⇒ per-anchor recovery without global scans.\n",
    "\n",
    "## Minimal Algorithm Sketch (Insert/Get)\n",
    "\n",
    "### Insert(k, v):\n",
    "```python\n",
    "d ← d_target()\n",
    "Loop:\n",
    "    a ← anchor(d, k)  # pack first d nibbles from HCS(k)\n",
    "    If size(a) < S_MAX: \n",
    "        place k in a; store (v, insert_depth=d); return\n",
    "    Else: \n",
    "        d ← d + 1  # local depth bump; no moves of existing keys\n",
    "```\n",
    "\n",
    "### Get(k):\n",
    "```python\n",
    "If anchor for k known: \n",
    "    return a[k]\n",
    "Else: \n",
    "    try d_target(), d_target()+1, … d_target()+Δ\n",
    "    on hit, backfill anchor and return\n",
    "    otherwise raise KeyError\n",
    "```\n",
    "\n",
    "## Implementation Options\n",
    "\n",
    "- **Per-anchor container:** single entry (pure 1-key anchors) or tiny microtable (e.g., 8–16 slots, open-addressed with 12–16-bit fingerprints) to reduce anchor count and improve cache/TLB locality.\n",
    "\n",
    "- **Anchor index:** direct map `(d, prefix)` → anchor, or a compact two-tier radix array for the top nibbles.\n",
    "\n",
    "- **Native core (C/Rust):** pack slots, use prefetch, branch-light probes; expose a Python shim for benchmarks.\n",
    "\n",
    "## When to choose HashChainAnchorDict\n",
    "\n",
    "- Need deterministic addressing, no global rehash, and predictable tail latency.\n",
    "- Care about persistence/sharding/self-healing (on-disk KV, verifiable stores).\n",
    "- Datasets are large or hot-spotty, where local depth bumps beat global resizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f59f8078-0246-4648-8934-b853086e8005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from itertools import chain as _it_chain\n",
    "\n",
    "class HashChainAnchorDict:\n",
    "    \"\"\"\n",
    "    Anchor-Hash-Chain dictionary with **1-hop lookups** and **local growth**.\n",
    "\n",
    "    This map anchors each key at insertion time using the first ``d`` nibbles of its\n",
    "    :class:`HashChainStream` (HCS). The anchor id is ``(depth=d, prefix)`` where\n",
    "    ``prefix`` is the packed integer of those nibbles. Lookups compute the same\n",
    "    anchor and jump there in one hop, then resolve from a small per-anchor table.\n",
    "\n",
    "    Key properties:\n",
    "        - **Deterministic addressing:** Address (anchor id) is a pure function of\n",
    "          ``(key, policy, HCS version)``.\n",
    "        - **No global rehash:** Growth is local; new inserts may use deeper anchors on\n",
    "          hot prefixes. Existing keys never move.\n",
    "        - **Self-healing:** Anchors can be rebuilt independently from keys (and, if\n",
    "          recorded, their ``insert_depth``), without scanning a monolithic table.\n",
    "\n",
    "    Typical usage:\n",
    "        >>> aht = HashChainAnchorDict(S_TARGET=16, S_MAX=32)\n",
    "        >>> aht[\"apple\"] = 1\n",
    "        >>> aht[\"banana\"] = 2\n",
    "        >>> aht[\"apple\"]\n",
    "        1\n",
    "        >>> aht.insert_depth(\"apple\")  # e.g., 1 or 2 depending on distribution\n",
    "        1\n",
    "        >>> len(aht)\n",
    "        2\n",
    "    \"\"\"\n",
    "\n",
    "    class _Anchor:\n",
    "        \"\"\"\n",
    "            Per-prefix container for the Anchor-Hash-Chain (AHC) dictionary.\n",
    "        \n",
    "            An `_Anchor` represents a **stable addressable region** identified by an\n",
    "            insertion depth ``depth`` (count of 4-bit nibbles consumed) and the packed\n",
    "            ``prefix`` of those nibbles. It owns a small, local map (``map``) that stores\n",
    "            all entries whose **creation prefix** equals ``(depth, prefix)``. Anchors are\n",
    "            stable: once created, they do not move even if nearby regions grow deeper.\n",
    "        \n",
    "            Attributes:\n",
    "                depth (int): The creation depth ``d`` (number of 4-bit links) used to\n",
    "                    place keys in this anchor. Keys stored here were created with exactly\n",
    "                    ``d`` links of their hash chain.\n",
    "                prefix (int): The packed big-endian integer formed from the first\n",
    "                    ``depth`` nibbles of the key’s :class:`HashChainStream`. For 4-bit links,\n",
    "                    this is a ``4*depth``-bit integer (e.g., ``depth=2``, nibbles ``[0xA,0x3]``\n",
    "                    → ``prefix=0xA3``).\n",
    "                map (dict): A **per-anchor microtable** mapping\n",
    "                    ``key -> (value, insert_depth, fingerprint16)`` where:\n",
    "                    - ``value``: stored payload\n",
    "                    - ``insert_depth`` (int): creation depth recorded for the key\n",
    "                    - ``fingerprint16`` (int): 16-bit fast-reject fingerprint\n",
    "                      (e.g., top 16 bits of ``block(0)``) to minimize equality checks.\n",
    "        \n",
    "            Notes:\n",
    "                - Slotted to minimize per-object overhead and improve cache locality.\n",
    "                - Enables **1-hop addressing**: given ``(depth, prefix)`` you can jump\n",
    "                  directly here without traversing from a global root.\n",
    "                - In sharded or on-disk deployments, each anchor can be persisted and\n",
    "                  repaired **independently**, enabling self-healing.\n",
    "            \"\"\"\n",
    "\n",
    "        # Only create these attributes for class instances\n",
    "        __slots__ = (\"depth\", \"prefix\", \"map\")\n",
    "        \n",
    "        def __init__(self, depth: int, prefix: int):\n",
    "            \"\"\"\n",
    "            Initialize a new per-prefix anchor.\n",
    "    \n",
    "            Args:\n",
    "                depth (int): Insertion depth ``d`` (number of 4-bit links). Must be ``>= 1``.\n",
    "                prefix (int): Packed prefix formed from the first ``depth`` nibbles of the\n",
    "                    key’s hash-chain stream. Must satisfy ``0 <= prefix < 1 << (4*depth)``.\n",
    "    \n",
    "            Sets:\n",
    "                depth: Stored as given.\n",
    "                prefix: Stored as given.\n",
    "                map: Empty per-anchor microtable ready for entries of the form\n",
    "                    ``key -> (value, insert_depth, fingerprint16)``.\n",
    "    \n",
    "            Raises:\n",
    "                ValueError: If ``depth < 1`` or ``prefix`` is out of range (optional to enforce).\n",
    "    \n",
    "            Examples:\n",
    "                >>> a = _Anchor(depth=2, prefix=0xA3)\n",
    "                >>> a.depth, hex(a.prefix)\n",
    "                (2, '0xa3')\n",
    "                >>> a.map[\"apple\"] = (42, 2, 0xBEEF)\n",
    "                >>> a.map[\"apple\"]\n",
    "                (42, 2, 48879)\n",
    "    \n",
    "            Design:\n",
    "                `_Anchor` instances correspond to **creation prefixes** and never move.\n",
    "                When an anchor’s container becomes crowded, **new** keys may be placed at\n",
    "                deeper anchors (larger ``depth``) by policy; existing keys remain in their\n",
    "                original anchor to avoid global reshuffles.\n",
    "            \"\"\"\n",
    "            self.depth = depth\n",
    "            self.prefix = prefix\n",
    "            self.map = {}  # key -> (value, insert_depth, fingerprint16)\n",
    "\n",
    "    def __init__(self, S_TARGET: int = 16, S_MAX: int = 32, MAX_D: int | None = None):\n",
    "        \"\"\"\n",
    "        Configure policy and initialize anchor state.\n",
    "\n",
    "        The policy sets a **global depth hint** and **per-anchor cap** used on insert.\n",
    "        From the current size ``N`` and desired average anchor size ``S_TARGET``, the\n",
    "        implementation computes:\n",
    "\n",
    "            ``E[size at depth d] ≈ N / 16^d ≤ S_TARGET``\n",
    "            ⇒ ``d_target = ceil((log2(N) − log2(S_TARGET)) / 4)``\n",
    "\n",
    "        New inserts start at ``d_target`` and **bump depth** while the chosen anchor\n",
    "        has ``size ≥ S_MAX``. Existing keys are never moved.\n",
    "\n",
    "        Args:\n",
    "            S_TARGET (int, optional): Desired **average** entries per anchor (guides\n",
    "                ``d_target``). Typical 8–32. Default: ``16``.\n",
    "            S_MAX (int, optional): **Hard cap** per anchor. If an anchor has\n",
    "                ``size ≥ S_MAX`` when inserting, increase depth and retry. Should be\n",
    "                ``>= S_TARGET``. Default: ``32``.\n",
    "            MAX_D (int | None, optional): Optional upper bound on depth. Default: ``None``.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If ``S_TARGET <= 0`` or ``S_MAX <= 0``.\n",
    "\n",
    "        Attributes:\n",
    "            S_TARGET (int): Stored average target per anchor.\n",
    "            S_MAX (int): Stored hard cap per anchor.\n",
    "            MAX_D (int | None): Maximum allowed depth.\n",
    "            _anchors (dict[(int,int), _Anchor]): Directory mapping ``(depth,prefix) → anchor``.\n",
    "            _key_anchor (dict[key, (int,int)]): Reverse map for **1-hop** lookups.\n",
    "            _size (int): Total number of key–value entries.\n",
    "        \"\"\"\n",
    "        if S_TARGET <= 0 or S_MAX <= 0:\n",
    "            raise ValueError(\"S_TARGET and S_MAX must be positive.\")\n",
    "        if S_MAX < S_TARGET:\n",
    "            # Usually S_MAX >= S_TARGET; warn or auto-adjust as needed.\n",
    "            pass\n",
    "\n",
    "        self.S_TARGET = S_TARGET\n",
    "        self.S_MAX = S_MAX\n",
    "        self.MAX_D = MAX_D\n",
    "\n",
    "        self._anchors: dict[tuple[int,int], HashChainDict._Anchor] = {}\n",
    "        self._key_anchor: dict[object, tuple[int,int]] = {}\n",
    "        self._size = 0\n",
    "\n",
    "    # ------------- helpers -------------\n",
    "\n",
    "    @staticmethod\n",
    "    def _prefix_of(key, d: int) -> int:\n",
    "        \"\"\"\n",
    "        Compute the packed prefix (big-endian) of the first ``d`` nibbles from\n",
    "        :class:`HashChainStream(key)`.\n",
    "\n",
    "        Args:\n",
    "            key: Key to encode.\n",
    "            d (int): Number of 4-bit nibbles to consume (``d >= 1``).\n",
    "\n",
    "        Returns:\n",
    "            int: Packed prefix in the range ``[0, 1 << (4*d))``.\n",
    "        \"\"\"\n",
    "        hc = HashChainStream(key)\n",
    "        out = 0\n",
    "        for i in range(d):\n",
    "            out = (out << 4) | hc.nibble(i)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def _fingerprint16(key) -> int:\n",
    "        \"\"\"\n",
    "        Compute a 16-bit fast-reject fingerprint for ``key``.\n",
    "\n",
    "        Implementation detail:\n",
    "            Common choice is the top 16 bits of ``block(0)`` from the key’s HCS.\n",
    "            This drastically reduces full key equality checks inside anchors.\n",
    "\n",
    "        Returns:\n",
    "            int: Fingerprint in ``[0, 65535]``.\n",
    "        \"\"\"\n",
    "        hc = HashChainStream(key)\n",
    "        return (hc.block(0) >> 48) & 0xFFFF\n",
    "\n",
    "    def _anchor_for(self, key, d: int):\n",
    "        \"\"\"\n",
    "        Get or create the anchor for ``key`` at depth ``d``.\n",
    "\n",
    "        Returns:\n",
    "            tuple[_Anchor, tuple[int,int]]: The anchor instance and its id\n",
    "            ``(depth, prefix)``.\n",
    "        \"\"\"\n",
    "        prefix = self._prefix_of(key, d)\n",
    "        ak = (d, prefix)\n",
    "        anc = self._anchors.get(ak)\n",
    "        if anc is None:\n",
    "            anc = self._Anchor(depth=d, prefix=prefix)\n",
    "            self._anchors[ak] = anc\n",
    "        return anc, ak\n",
    "\n",
    "    def _d_target(self) -> int:\n",
    "        \"\"\"\n",
    "        Compute the current **target depth** ``d_target`` from size ``N`` and\n",
    "        policy parameter ``S_TARGET``:\n",
    "\n",
    "            ``d_target = ceil((log2(max(1,N)) - log2(S_TARGET)) / 4)``\n",
    "\n",
    "        The result is clamped to ``>= 1`` and to ``<= MAX_D`` when set.\n",
    "\n",
    "        Returns:\n",
    "            int: Suggested starting depth for new inserts.\n",
    "        \"\"\"\n",
    "        N = max(1, self._size)  # avoid log(0)\n",
    "        d = math.ceil((math.log2(N) - math.log2(self.S_TARGET)) / 4.0)\n",
    "        d = max(1, d)\n",
    "        if self.MAX_D is not None:\n",
    "            d = min(d, self.MAX_D)\n",
    "        return d\n",
    "\n",
    "    # ------------- public API -------------\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        \"\"\"\n",
    "        Insert or update ``key → value``.\n",
    "\n",
    "        Behavior:\n",
    "            - **Update fast path:** If the key’s anchor is known, overwrite in place.\n",
    "            - **Insert path:** Start at ``d_target``; if the chosen anchor is at/over\n",
    "              ``S_MAX`` entries, increase depth and retry. Record ``insert_depth``\n",
    "              for the key at the final anchor.\n",
    "\n",
    "        Complexity:\n",
    "            Expected O(1). Depth bumps are local and do not move existing keys.\n",
    "\n",
    "        Examples:\n",
    "            >>> aht = HashChainAnchorDict()\n",
    "            >>> aht[\"k1\"] = 123\n",
    "            >>> aht[\"k1\"] = 456  # update\n",
    "        \"\"\"\n",
    "        # Fast path: overwrite if we already know the anchor\n",
    "        ak = self._key_anchor.get(key)\n",
    "        if ak is not None:\n",
    "            anc = self._anchors.get(ak)\n",
    "            if anc is not None:\n",
    "                fp = self._fingerprint16(key)\n",
    "                _, ins_d, _ = anc.map.get(key, (None, anc.depth, None))\n",
    "                anc.map[key] = (value, ins_d, fp)\n",
    "                return\n",
    "            else:\n",
    "                # Anchor missing unexpectedly; fall through to fresh insert path\n",
    "                self._key_anchor.pop(key, None)\n",
    "\n",
    "        # Fresh insert with policy: start at d_target, bump while anchor is crowded\n",
    "        fp = self._fingerprint16(key)\n",
    "        d = self._d_target()\n",
    "        while True:\n",
    "            if self.MAX_D is not None and d > self.MAX_D:\n",
    "                # last resort: pin to MAX_D\n",
    "                d = self.MAX_D\n",
    "            anc, ak = self._anchor_for(key, d)\n",
    "            # If key already present, update and bind anchor\n",
    "            if key in anc.map:\n",
    "                _, ins_d, _ = anc.map[key]\n",
    "                anc.map[key] = (value, ins_d, fp)\n",
    "                self._key_anchor[key] = ak\n",
    "                return\n",
    "            # If anchor too large, push deeper and try again\n",
    "            if len(anc.map) >= self.S_MAX:\n",
    "                d += 1\n",
    "                continue\n",
    "            # Place here; record insert_depth = d\n",
    "            anc.map[key] = (value, d, fp)\n",
    "            self._key_anchor[key] = ak\n",
    "            self._size += 1\n",
    "            return\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Retrieve the value for ``key`` or raise :class:`KeyError`.\n",
    "\n",
    "        Lookup:\n",
    "            1. Use the recorded anchor for **1-hop** jump when available.\n",
    "            2. Otherwise, perform a small bounded probe across plausible depths\n",
    "               near the current ``d_target`` and backfill the anchor on hit.\n",
    "\n",
    "        Returns:\n",
    "            Any: The stored value.\n",
    "\n",
    "        Raises:\n",
    "            KeyError: If ``key`` is not present.\n",
    "\n",
    "        Examples:\n",
    "            >>> aht = HashChainAnchorDict()\n",
    "            >>> aht[\"x\"] = 1\n",
    "            >>> aht[\"x\"]\n",
    "            1\n",
    "        \"\"\"\n",
    "        ak = self._key_anchor.get(key)\n",
    "        if ak is not None:\n",
    "            anc = self._anchors.get(ak)\n",
    "            if anc is not None:\n",
    "                entry = anc.map.get(key)\n",
    "                if entry is not None:\n",
    "                    value, _ins_d, _fp = entry\n",
    "                    return value\n",
    "        # Fallback: bounded probe across plausible depths near current d_target\n",
    "        # (helps if caller forgot to use our insert path or metadata was lost)\n",
    "        d0 = self._d_target()\n",
    "        for delta in range(0, 5):  # try d0, d0+1, ..., d0+4\n",
    "            d_try = d0 + delta\n",
    "            anc, ak2 = self._anchor_for(key, d_try)\n",
    "            entry = anc.map.get(key)\n",
    "            if entry is not None:\n",
    "                value, _ins_d, _fp = entry\n",
    "                self._key_anchor[key] = ak2  # backfill for 1-hop next time\n",
    "                return value\n",
    "        raise KeyError(key)\n",
    "\n",
    "    def __delitem__(self, key):\n",
    "        \"\"\"\n",
    "        Delete ``key`` or raise :class:`KeyError`.\n",
    "\n",
    "        Removes the entry from its anchor and updates internal counts. No rehash or\n",
    "        global movement occurs; only the target anchor is modified.\n",
    "\n",
    "        Raises:\n",
    "            KeyError: If ``key`` is not present.\n",
    "\n",
    "        Examples:\n",
    "            >>> aht = HashChainAnchorDict()\n",
    "            >>> aht[\"x\"] = 1\n",
    "            >>> del aht[\"x\"]\n",
    "            >>> \"x\" in list(aht.keys())\n",
    "            False\n",
    "        \"\"\"\n",
    "        ak = self._key_anchor.get(key)\n",
    "        if ak is None:\n",
    "            # Try small neighborhood\n",
    "            d0 = self._d_target()\n",
    "            for delta in range(0, 5):\n",
    "                d_try = d0 + delta\n",
    "                anc, _ = self._anchor_for(key, d_try)\n",
    "                if key in anc.map:\n",
    "                    del anc.map[key]\n",
    "                    self._size -= 1\n",
    "                    return\n",
    "            raise KeyError(key)\n",
    "        anc = self._anchors.get(ak)\n",
    "        if anc is None or key not in anc.map:\n",
    "            raise KeyError(key)\n",
    "        del anc.map[key]\n",
    "        self._key_anchor.pop(key, None)\n",
    "        self._size -= 1\n",
    "\n",
    "    # Basics\n",
    "    def __len__(self): \n",
    "        \"\"\"\n",
    "        Return the number of entries in the dictionary.\n",
    "\n",
    "        Returns:\n",
    "            int: Total key–value pairs across all anchors.\n",
    "        \"\"\"        \n",
    "        return self._size\n",
    "        \n",
    "    def __iter__(self): \n",
    "        \"\"\"\n",
    "        Iterate over keys.\n",
    "\n",
    "        Equivalent to :meth:`keys`. Yields each key exactly once.\n",
    "        \"\"\"\n",
    "        return iter(self.keys())\n",
    "\n",
    "    # Iteration helpers\n",
    "    def keys(self):\n",
    "        \"\"\"\n",
    "        Iterate over keys across all anchors.\n",
    "\n",
    "        Yields:\n",
    "            The keys present in the map (iteration order is unspecified).\n",
    "        \"\"\"\n",
    "        return _it_chain.from_iterable(anc.map.keys() for anc in self._anchors.values())\n",
    "\n",
    "    def values(self):\n",
    "        \"\"\"\n",
    "        Iterate over values across all anchors.\n",
    "\n",
    "        Yields:\n",
    "            The stored values (iteration order follows :meth:`keys`).\n",
    "        \"\"\"\n",
    "        return _it_chain.from_iterable((v for (v, _insd, _fp) in anc.map.values())\n",
    "                                       for anc in self._anchors.values())\n",
    "\n",
    "    def items(self):\n",
    "        \"\"\"\n",
    "        Iterate over ``(key, value)`` pairs across all anchors.\n",
    "\n",
    "        Yields:\n",
    "            tuple: ``(key, value)`` for each entry.\n",
    "        \"\"\"\n",
    "        for anc in self._anchors.values():\n",
    "            for k, (v, _insd, _fp) in anc.map.items():\n",
    "                yield (k, v)\n",
    "\n",
    "    # Metadata\n",
    "    def insert_depth(self, key) -> int:\n",
    "        \"\"\"\n",
    "        Return the recorded **insert depth** (``d``) for ``key``.\n",
    "\n",
    "        This is the number of 4-bit links consumed at the time the key was placed.\n",
    "        It is used to compute the exact anchor id for **1-hop** lookups and enables\n",
    "        forensics/analytics on depth distribution.\n",
    "\n",
    "        Returns:\n",
    "            int: Creation depth for ``key``.\n",
    "\n",
    "        Raises:\n",
    "            KeyError: If ``key`` is not present.\n",
    "        \"\"\"\n",
    "        ak = self._key_anchor.get(key)\n",
    "        if ak is None:\n",
    "            raise KeyError(key)\n",
    "        anc = self._anchors[ak]\n",
    "        _, ins_d, _ = anc.map[key]\n",
    "        return ins_d\n",
    "\n",
    "    def anchor_id(self, key):\n",
    "        \"\"\"\n",
    "        Return the anchor id for ``key`` as ``(depth, prefix_int)``.\n",
    "\n",
    "        Useful for debugging, persistence, or sharding by anchor.\n",
    "\n",
    "        Returns:\n",
    "            tuple[int, int]: ``(depth, prefix)`` for ``key``.\n",
    "\n",
    "        Raises:\n",
    "            KeyError: If ``key`` is not present.\n",
    "        \"\"\"\n",
    "        ak = self._key_anchor.get(key)\n",
    "        if ak is None:\n",
    "            raise KeyError(key)\n",
    "        return ak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5275937-9e85-4912-b695-49df73d58f1b",
   "metadata": {},
   "source": [
    "### HashChainAnchorDict Examples\n",
    "\n",
    "The following examples demonstrate how to use a `HashChainAnchorDict` much like a Python `dict`, \n",
    "while benefiting from its deterministic, nibble-by-nibble trie structure.\n",
    "\n",
    "#### Basic Insertion & Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fef7e75f-34a2-4b0a-9526-2b459be2b26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "m = HashChainAnchorDict()\n",
    "m[\"alpha\"] = 1\n",
    "m[\"beta\"] = 2\n",
    "\n",
    "print(len(m))         # → 2\n",
    "print(m[\"alpha\"])     # → 1\n",
    "print(\"beta\" in m)    # → True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646edebb-fe6d-43cb-8fac-78a3dba6855f",
   "metadata": {},
   "source": [
    "#### Updating an Existing Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fb93ac83-a957-411b-9d46-7f5dfba9ddf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "m[\"alpha\"] = 42\n",
    "print(m[\"alpha\"])     # → 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e12ed3-f766-4a21-92b6-35040054c24f",
   "metadata": {},
   "source": [
    "#### Deletion with Pruning\n",
    "\n",
    "**Implementation note:** after deletion, the anchor remains but is empty; the current code keeps the empty anchor (cheap in RAM). If you want aggressive cleanup, you can add a check to drop anchors whose map becomes empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ad35c495-a9e6-445d-9770-042d388f6148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "del m[\"beta\"]\n",
    "print(\"beta\" in m)    # → False\n",
    "print(len(m))         # → 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f31e5bd-efe8-4289-8e1b-94126c3ecdb7",
   "metadata": {},
   "source": [
    "#### Iterating Keys, Values, Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a2b5cccd-8ae4-4bec-935a-65b8e36626c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alpha', 'gamma', 'delta']\n",
      "[42, 3, 4]\n",
      "[('alpha', 42), ('gamma', 3), ('delta', 4)]\n"
     ]
    }
   ],
   "source": [
    "m[\"gamma\"] = 3\n",
    "m[\"delta\"] = 4\n",
    "\n",
    "print(list(m.keys()))      # → ['alpha', 'gamma', 'delta']  (order not guaranteed)\n",
    "print(list(m.values()))    # → [42, 3, 4]\n",
    "print(list(m.items()))     # → [('alpha', 42), ('gamma', 3), ('delta', 4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847c0f5e-7ddf-4959-a650-596abe0d4820",
   "metadata": {},
   "source": [
    "#### Deterministic Paths\n",
    "\n",
    "Because each key’s path is determined by its hash-chain nibble sequence,\n",
    "prefix stability is guaranteed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5d1bf0cb-6c33-414a-9543-b05c8ca44f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "afc31693714b7c1c\n",
      "1 0xa\n"
     ]
    }
   ],
   "source": [
    "m = HashChainAnchorDict()\n",
    "m[\"x\"] = 100\n",
    "hx = HashChainStream(\"x\").take_hex(16)\n",
    "print(hx) \n",
    "# This hex string represents the deterministic nibble path for 'x'\n",
    "\n",
    "d = m.insert_depth(\"x\")                 # creation depth\n",
    "prefix = m._prefix_of(\"x\", d)           # packed prefix at that depth\n",
    "print(d, hex(prefix))                   # e.g., 1 0xa  (example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f36388c-c7e2-4a4d-b430-22e83443f016",
   "metadata": {},
   "source": [
    "#### Collision Handling\n",
    "\n",
    "Keys with long common prefixes in their hash-chain output will share the **same anchor candidate** (same `(depth, prefix)`) until their nibble sequences diverge. In **HashChainAnchorDict**, collisions are resolved **locally** by increasing depth (consuming one more 4-bit nibble) for the *new* key only—**existing keys never move**. This yields deterministic placement without global rehashes.\n",
    "\n",
    "- **Prefix stability:** Earlier nibbles for a key never change, so prior anchors remain valid.\n",
    "- **Local growth:** Only hot prefixes deepen; unrelated regions are untouched.\n",
    "- **1-hop lookups:** Once a key is anchored, lookups jump directly to its anchor and probe a tiny microtable (or single slot).\n",
    "\n",
    "---\n",
    "\n",
    "#### Visual Walk-through of Anchor Placement\n",
    "\n",
    "Suppose we insert two keys `\"cat\"` and `\"car\"`. Their hash-chain nibbles may start like this:\n",
    "\n",
    "- `HashChainStream(\"cat\").take_hex(4) → a3f1`\n",
    "- `HashChainStream(\"car\").take_hex(4) → a3d9`\n",
    "\n",
    "Both share prefix `a3` but diverge at the next nibble (`f` vs `d`). If `d_target=2`, both will initially target anchors with `depth=2` and `prefix=a3`. When the second insert collides, the newcomer bumps to `depth=3`:\n",
    "\n",
    "**Anchors (conceptual)**\n",
    "* (d=2, prefix=a3) → { \"cat\", ... }\n",
    "* (d=3, prefix=a3d) → { \"car\" }\n",
    "\n",
    "\n",
    "**Key points**\n",
    "- The **first differing nibble** determines how far the new key needs to go.\n",
    "- Existing keys at `(d=2, prefix=a3)` stay put; the new key goes to `(d=3, prefix=a3d)`.\n",
    "- Future lookups are **1 hop**: `(d, prefix)` → anchor → value.\n",
    "\n",
    "---\n",
    "\n",
    "### Programmatic Collision Demo (with real keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "14e7c8cc-ee06-4cd3-80e2-8a31bc86a3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found collision on first 3 nibbles (algo=blake2b):\n",
      "'demo-14' → cf78284e  (nibbles: c f 7 8 2 8 4 e)\n",
      "'demo-48' → cf77adde  (nibbles: c f 7 7 a d d e)\n",
      "Shared prefix (first 3 nibbles): cf7\n",
      "First divergence at nibble index d = 3\n",
      "demo-14: depth=1, prefix=0xc\n",
      "demo-48: depth=1, prefix=0xc\n",
      "Lookup:\n",
      "demo-14 → value1\n",
      "demo-48 → value2\n",
      "'demo-14' → cf78284e21a57bd7  (nibbles: c f 7 8 2 8 4 e 2 1 a 5 7 b d 7)\n",
      "'demo-48' → cf77adde8bedeec8  (nibbles: c f 7 7 a d d e 8 b e d e e c 8)\n"
     ]
    }
   ],
   "source": [
    "# --- Collision demo utilities ---\n",
    "\n",
    "from typing import Tuple, Optional, Dict\n",
    "from itertools import count\n",
    "\n",
    "# Adjust imports to your module layout\n",
    "# from your_module import HashChainStream, RadixHashChainMap\n",
    "\n",
    "def nibble_prefix_hex(key, L: int, *, algo=\"blake2b\"):\n",
    "    \"\"\"Return the first L nibbles of the key's stream as hex.\"\"\"\n",
    "    return HashChainStream(key, algo=algo).take_hex(L)\n",
    "\n",
    "def find_prefix_collision(L: int, *, algo=\"blake2b\", seed=\"k\", limit=200_000) -> Tuple[str, str, str]:\n",
    "    \"\"\"\n",
    "    Find two distinct strings seed+N that share the same first L nibbles\n",
    "    under the chosen PRF. Returns (k1, k2, shared_prefix).\n",
    "    \"\"\"\n",
    "    seen: Dict[str, str] = {}\n",
    "    for n in count():\n",
    "        k = f\"{seed}{n}\"\n",
    "        pref = nibble_prefix_hex(k, L, algo=algo)\n",
    "        if pref in seen and seen[pref] != k:\n",
    "            return seen[pref], k, pref\n",
    "        seen[pref] = seen.get(pref, k)\n",
    "        if n >= limit:\n",
    "            raise RuntimeError(f\"No collision found for L={L} within {limit} tries (algo={algo})\")\n",
    "\n",
    "def first_divergence(k1, k2, *, algo=\"blake2b\", max_nibbles=128) -> int:\n",
    "    \"\"\"\n",
    "    Return the nibble index at which the two keys first differ.\n",
    "    If identical up to max_nibbles, returns max_nibbles.\n",
    "    \"\"\"\n",
    "    h1 = HashChainStream(k1, algo=algo)\n",
    "    h2 = HashChainStream(k2, algo=algo)\n",
    "    for d in range(max_nibbles):\n",
    "        if h1.nibble(d) != h2.nibble(d):\n",
    "            return d\n",
    "    return max_nibbles\n",
    "\n",
    "def show_path(key, n=16, *, algo=\"blake2b\"):\n",
    "    \"\"\"Pretty-print the first n nibbles of a key's path.\"\"\"\n",
    "    hx = HashChainStream(key, algo=algo).take_hex(n)\n",
    "    print(f\"{key!r} → {hx}  (nibbles: {' '.join(hx)})\")\n",
    "\n",
    "# --- Find an actual collision and insert into the map ---\n",
    "\n",
    "L = 3                 # number of shared prefix nibbles to require (tweak 2..4)\n",
    "ALGO = \"blake2b\"      # or \"blake3\", \"xxh3\", \"xxh64\"\n",
    "\n",
    "k1, k2, pref = find_prefix_collision(L, algo=ALGO, seed=\"demo-\")\n",
    "print(f\"Found collision on first {L} nibbles (algo={ALGO}):\")\n",
    "show_path(k1, n=8, algo=ALGO)\n",
    "show_path(k2, n=8, algo=ALGO)\n",
    "print(f\"Shared prefix (first {L} nibbles): {pref}\")\n",
    "\n",
    "div = first_divergence(k1, k2, algo=ALGO, max_nibbles=64)\n",
    "print(f\"First divergence at nibble index d = {div}\")\n",
    "\n",
    "# Build the map and insert both keys\n",
    "m = HashChainAnchorDict()\n",
    "m[k1] = \"value1\"\n",
    "m[k2] = \"value2\"\n",
    "\n",
    "d1 = m.insert_depth(k1); p1 = m._prefix_of(k1, d1)\n",
    "d2 = m.insert_depth(k2); p2 = m._prefix_of(k2, d2)\n",
    "print(f\"{k1}: depth={d1}, prefix=0x{p1:x}\")\n",
    "print(f\"{k2}: depth={d2}, prefix=0x{p2:x}\")\n",
    "\n",
    "print(\"Lookup:\")\n",
    "print(k1, \"→\", m[k1])\n",
    "print(k2, \"→\", m[k2])\n",
    "\n",
    "show_path(k1, n=16, algo=ALGO)\n",
    "show_path(k2, n=16, algo=ALGO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38aa294-fda0-4896-b72c-98e8a033e8e4",
   "metadata": {},
   "source": [
    "### Explanation of observed outputs (example run above):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bbb6e2-4c0e-4412-9f84-3ba5c82c78cf",
   "metadata": {},
   "source": [
    "**Collision search results:**\n",
    "- Found a collision on the first **3 nibbles** using `blake2b`.\n",
    "- Both keys share the prefix `cf7`.\n",
    "    \n",
    "**Anchor placement:**\n",
    "- Both keys initially target depth `1` (prefix `0xc`).\n",
    "- Depending on anchor capacity, a local depth bump may occur for one of them.  \n",
    "- In this run, both report `depth=1, prefix=0xc`.\n",
    "\n",
    "**Lookup results:**\n",
    "- Retrieval works in **1 hop** to the anchor + direct key probe.\n",
    "\n",
    "**Extended nibble paths (16 nibbles each):**\n",
    " * 'demo-14' → cf78284e21a57bd7 (nibbles: c f 7 8 2 8 4 e 2 1 a 5 7 b d 7)   \n",
    " * 'demo-48' → cf77adde8bedeec8 (nibbles: c f 7 7 a d d e 8 b e d e e c 8)\n",
    "\n",
    "**Interpretation:**\n",
    "- Both keys align for the first 3 nibbles (`c f 7`) and then diverge at the 4th nibble (`8` vs `7`).\n",
    "- The deterministic hash-chain stream guarantees these prefixes are stable across rebuilds.\n",
    "- Collisions are resolved **locally** by deepening anchors when necessary, without moving earlier keys.                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cf9b48-822b-412a-bed9-84eb82e30982",
   "metadata": {},
   "source": [
    "### Recompute to Assert Behavior (adjust `ALGO` / keys as needed)\n",
    "\n",
    "Use the exact keys found by the collision demo to **verify** the shared prefix,\n",
    "the first divergence, and correct map behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e8d72422-7e3f-4753-9340-f6d5626a71b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shared 3-nibble prefix: cf7\n",
      "nibble @ d=3: 8 (for demo-14), 7 (for demo-48)\n",
      "RadixHashChainMap lookups OK: value1 value2 | len = 2\n",
      "AnchorDict(default) lookups: value1 value2\n",
      "Default caps: demo-14 at (d=1, prefix=0xc); demo-48 at (d=1, prefix=0xc)\n",
      "AnchorDict(forced) lookups: value1 value2\n",
      "Forced caps:  demo-14 at (d=1, prefix=0xc); demo-48 at (d=2, prefix=0xcf)\n"
     ]
    }
   ],
   "source": [
    "# --- Collision-aware demo using the found keys ---\n",
    "\n",
    "ALGO = \"blake2b\"\n",
    "k1, k2 = \"demo-14\", \"demo-48\"\n",
    "\n",
    "# Hash streams\n",
    "h1 = HashChainStream(k1, algo=ALGO)\n",
    "h2 = HashChainStream(k2, algo=ALGO)\n",
    "\n",
    "# 1) Verify the observed collision properties\n",
    "assert h1.take_hex(3) == h2.take_hex(3) == \"cf7\", \"Expected shared 3-nibble prefix 'cf7'\"\n",
    "for d in range(3):\n",
    "    assert h1.nibble(d) == h2.nibble(d), f\"Mismatch at nibble {d}\"\n",
    "assert h1.nibble(3) != h2.nibble(3), \"Expected divergence at nibble index 3\"\n",
    "\n",
    "print(\"Shared 3-nibble prefix:\", h1.take_hex(3))\n",
    "print(\"nibble @ d=3:\", h1.nibble(3), \"(for demo-14),\", h2.nibble(3), \"(for demo-48)\")\n",
    "\n",
    "# 2) RadixHashChainMap behavior (trie-style split at first differing nibble)\n",
    "rm = RadixHashChainMap()\n",
    "rm[k1] = \"value1\"\n",
    "rm[k2] = \"value2\"\n",
    "assert rm[k1] == \"value1\"\n",
    "assert rm[k2] == \"value2\"\n",
    "assert len(rm) == 2\n",
    "print(\"RadixHashChainMap lookups OK:\", rm[k1], rm[k2], \"| len =\", len(rm))\n",
    "\n",
    "# 3) HashChainAnchorDict behavior (anchor-style, 1-hop lookups)\n",
    "#    Case A: default caps (both keys may live in the same (d,prefix) anchor if under S_MAX)\n",
    "am_default = HashChainAnchorDict()  # default S_MAX allows multiple in one anchor\n",
    "am_default[k1] = \"value1\"\n",
    "am_default[k2] = \"value2\"\n",
    "print(\"AnchorDict(default) lookups:\", am_default[k1], am_default[k2])\n",
    "\n",
    "d1 = am_default.insert_depth(k1); p1 = am_default._prefix_of(k1, d1)\n",
    "d2 = am_default.insert_depth(k2); p2 = am_default._prefix_of(k2, d2)\n",
    "print(f\"Default caps: {k1} at (d={d1}, prefix=0x{p1:x}); {k2} at (d={d2}, prefix=0x{p2:x})\")\n",
    "\n",
    "#    Case B: force a local depth bump by capping anchor size to 1\n",
    "am_forced = HashChainAnchorDict(S_TARGET=1, S_MAX=1)\n",
    "am_forced[k1] = \"value1\"\n",
    "am_forced[k2] = \"value2\"\n",
    "print(\"AnchorDict(forced) lookups:\", am_forced[k1], am_forced[k2])\n",
    "\n",
    "fd1 = am_forced.insert_depth(k1); fp1 = am_forced._prefix_of(k1, fd1)\n",
    "fd2 = am_forced.insert_depth(k2); fp2 = am_forced._prefix_of(k2, fd2)\n",
    "print(f\"Forced caps:  {k1} at (d={fd1}, prefix=0x{fp1:x}); {k2} at (d={fd2}, prefix=0x{fp2:x})\")\n",
    "\n",
    "# Sanity:\n",
    "# - In default caps, both keys may share the same (d, prefix) if the anchor has room.\n",
    "# - In forced caps (S_MAX=1), the second insert must deepen (fd2 >= fd1, and top nibble matches).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace92454-7d9c-4a6e-8fe7-c0218e7e6122",
   "metadata": {},
   "source": [
    "### Explanation of Observed Outputs (example run above)\n",
    "\n",
    "#### Shared 3-nibble prefix: cf7\n",
    "\n",
    "The two keys (\"demo-14\", \"demo-48\") have identical first three nibbles from their HashChainStream. This confirms a controlled prefix collision suitable for demonstrating local resolution.\n",
    "\n",
    "#### nibble @ d=3: 8 (for demo-14), 7 (for demo-48)\n",
    "\n",
    "At nibble index d=3 (the 4th nibble), the streams diverge (8 vs 7). This is the earliest position where the keys’ paths differ and is the depth at which a trie would branch or an anchor policy might deepen.\n",
    "\n",
    "#### RadixHashChainMap lookups OK: value1 value2 | len = 2\n",
    "\n",
    "The radix (trie) variant splits exactly at the first differing nibble and stores both keys in separate leaves. Lookups succeed; the map contains two entries.\n",
    "\n",
    "#### AnchorDict(default) lookups: value1 value2\n",
    "\n",
    "With default per-anchor capacity, both keys can still be served correctly. The anchor design allows multiple keys to coexist in the same anchor if it hasn’t reached its capacity.\n",
    "\n",
    "#### Default caps: demo-14 at (d=1, prefix=0xc); demo-48 at (d=1, prefix=0xc)\n",
    "\n",
    "Under the default policy, both keys anchored at depth 1 with the same first nibble (0xc). No local depth bump was required because the anchor’s microtable had room. Existing keys never move; new keys join the same anchor when capacity allows.\n",
    "\n",
    "#### AnchorDict(forced) lookups: value1 value2\n",
    "\n",
    "When we constrain anchors to one entry (S_MAX=1), lookups still succeed—this setting is purely to demonstrate collision handling under pressure.\n",
    "\n",
    "#### Forced caps: demo-14 at (d=1, prefix=0xc); demo-48 at (d=2, prefix=0xcf)\n",
    "\n",
    "With S_MAX=1, inserting the second colliding key forces a local depth bump: the newcomer deepens to (d=2) and uses the next nibble (0xf) to select a different anchor. \n",
    "\n",
    "**This illustrates the anchor policy:** \n",
    "\n",
    "Collisions are resolved by deepening only the new key, with no global rehash and no movement of already-inserted keys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86944b4a-296e-4a5a-b22e-d40658433e9c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# HashChainAnchorDict vs. Python Dict\n",
    "\n",
    "## 1. What's the problem with traditional dict\n",
    "\n",
    "In a normal dict or flat hash table, the placement of a key depends on:\n",
    "\n",
    "- The full hash value\n",
    "- The current table size (mask, modulus)\n",
    "- The probing scheme (linear, quadratic, robin-hood…)\n",
    "\n",
    "If the table is resized or partially corrupted, you cannot regenerate the layout from just the keys. You need to re-insert everything or scan the entire monolithic table to rebuild it.\n",
    "\n",
    "**Worse:** if a directory page is missing (e.g., a shard file corrupted), you have no way to self-heal from just the surviving pieces — you have to rebuild globally.\n",
    "\n",
    "## 2. Why Anchor-Hash-Chain (HAD) is different\n",
    "\n",
    "The core property is that every key has a **deterministic, prefix-stable address:**\n",
    "\n",
    "- A key `k` always yields the same nibble sequence from its HashChainStream\n",
    "- At insertion, we stop at some depth `d` and create the anchor `(d, prefix(k, d))`\n",
    "- That `(d, prefix)` never changes, even if the table grows or other anchors split deeper\n",
    "- Thus the anchor ID acts like a **permanent address** for the key\n",
    "\n",
    "So the anchor directory is just a mapping `(d, prefix) → container` — and containers never move; only new inserts go deeper when a container is crowded.\n",
    "\n",
    "## 3. What \"self-healing\" means\n",
    "\n",
    "### Rebuild anchors from keys directly\n",
    "Suppose you lose part of the directory, or you start from scratch with only the keys:\n",
    "\n",
    "1. For each key, recompute its insert depth `d` (if you stored it; otherwise recompute via policy) and the prefix `prefix(k, d)`\n",
    "2. Put the key back into the exact same anchor without scanning all other anchors\n",
    "\n",
    "### Distributed repair (anchors are independent):\n",
    "- If one anchor shard is missing, you only need to touch the keys that belonged there\n",
    "- Other anchors are unaffected\n",
    "- **Contrast:** a dict resize touches all slots\n",
    "\n",
    "### Error correction / duplicates\n",
    "If a container file gets corrupted, regenerate it by walking only the keys that map to that `(d, prefix)` and re-inserting them into a fresh container.\n",
    "\n",
    "→ **No global scan required.**\n",
    "\n",
    "## 4. How you know the \"insert depth\" again\n",
    "\n",
    "**Option A (metadata stored):** record `insert_depth = d` at insert time (one byte per key). During rebuild, use it directly.\n",
    "\n",
    "**Option B (policy re-execution):** run the same placement policy you used originally (e.g., \"start at `d_target`, bump `d` while container size ≥ `S_MAX`\"). Because the hash chain is deterministic, you land the key in the same anchor again.\n",
    "\n",
    "Either way, anchor assignment is reproducible without global state.\n",
    "\n",
    "## 5. Why you don't need a monolithic scan\n",
    "\n",
    "| Approach | Recovery Method | Cost |\n",
    "|----------|----------------|------|\n",
    "| **Flat hash table recovery** | Read all slots, skip empties, rehash every key into a fresh table | O(N) always |\n",
    "| **Anchor-Hash-Chain recovery** | For the affected anchors only, recompute prefixes for their keys. Other anchors remain untouched | O(size of damaged shard), not O(N) |\n",
    "\n",
    "## 6. Practical persistence story\n",
    "\n",
    "Imagine you store each anchor as a file (or a record in a CAS repository):\n",
    "\n",
    "- **Filename** = `(d, prefix)` (e.g., `2_0xA3`)\n",
    "- **Contents** = small microtable of entries\n",
    "\n",
    "If file `2_0xA3` is lost or corrupted:\n",
    "1. Recompute keys with prefix `0xA3` at depth `2`\n",
    "2. Re-write the file\n",
    "3. Done. The rest of the system didn't move, so no global rebuild\n",
    "\n",
    "## 7. Why this is powerful\n",
    "\n",
    "- **Crash resilience:** a torn write damages one anchor, not the whole map\n",
    "- **Incremental repair:** restore selectively; no need to re-ingest the entire dataset\n",
    "- **Versioning:** anchors are stable, so you can diff them between snapshots\n",
    "- **Distributed storage:** anchors can be spread across nodes/disks and recomputed independently\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "**Self-healing is possible** because a key's anchor address `(d, prefix)` is intrinsic to the key + policy, not to a mutable global table.\n",
    "\n",
    "You can rebuild any damaged anchor shard in isolation, using only the keys (and optionally the 1-byte `insert_depth`), without scanning or rehashing the entire dataset.\n",
    "\n",
    "This enables incremental repair, fast recovery, and tamper-evident persistence in large, distributed, or on-disk deployments.\n",
    "\n",
    "---\n",
    "\n",
    "# Practical Example\n",
    "\n",
    "## Setup\n",
    "\n",
    "- **Fanout per link:** 16 (4-bit nibbles)\n",
    "- **Anchors:** `(d, prefix)` where prefix is the first `d` nibbles of `HashChainStream(key)`\n",
    "- **Policy (simple):** start at `d=1`; if an anchor's container reaches `S_MAX=3` entries, bump `d` for new inserts (existing entries never move)\n",
    "\n",
    "Assume the hash-chain nibbles for our five keys are:\n",
    "\n",
    "| Key | First 3 nibbles (L0, L1, L2) |\n",
    "|-----|------------------------------|\n",
    "| A   | A, 3, 7                      |\n",
    "| B   | A, 3, 4                      |\n",
    "| C   | A, 6, 1                      |\n",
    "| D   | 9, 2, B                      |\n",
    "| E   | A, 3, 2                      |\n",
    "\n",
    "*(These come deterministically from HashChainStream(key); shown as hex nibbles.)*\n",
    "\n",
    "## Insert phase (building the table)\n",
    "\n",
    "### Insert A\n",
    "- Try `d=1`: prefix = `A`\n",
    "- Anchor `(1, A)` doesn't exist → create it with a small container (`S_MAX=3`)\n",
    "- Place A there. Record `insert_depth(A)=1`\n",
    "\n",
    "**Anchor table now:**\n",
    "```\n",
    "(1, A) → {A}\n",
    "```\n",
    "\n",
    "### Insert B (A,3,…)\n",
    "- Try `d=1`: `(1, A)` size = 1 < `S_MAX`\n",
    "- Place B in `(1, A)`. `insert_depth(B)=1`\n",
    "\n",
    "**Anchor table:**\n",
    "```\n",
    "(1, A) → {A, B}\n",
    "```\n",
    "\n",
    "### Insert C (A,6,…)\n",
    "- Try `d=1`: `(1, A)` size = 2 < `S_MAX`\n",
    "- Place C in `(1, A)`. `insert_depth(C)=1`\n",
    "\n",
    "**Anchor table:**\n",
    "```\n",
    "(1, A) → {A, B, C} (size = 3, now at cap)\n",
    "```\n",
    "\n",
    "### Insert D (9,2,…)\n",
    "- Try `d=1`: `(1, 9)` doesn't exist → create and place D\n",
    "- `insert_depth(D)=1`\n",
    "\n",
    "**Anchor table:**\n",
    "```\n",
    "(1, A) → {A, B, C}\n",
    "(1, 9) → {D}\n",
    "```\n",
    "\n",
    "### Insert E (A,3,2)\n",
    "- Try `d=1`: `(1, A)` size = 3 == `S_MAX` → push deeper for this insert only\n",
    "- Try `d=2`: prefix = `A3`. `(2, A3)` doesn't exist → create and place E\n",
    "- `insert_depth(E)=2`\n",
    "\n",
    "## Final anchor table after inserts\n",
    "\n",
    "```\n",
    "(1, A) → {A, B, C} (all inserted when shallow)\n",
    "(1, 9) → {D}\n",
    "(2, A3) → {E} (inserted deeper due to cap)\n",
    "```\n",
    "\n",
    "**Note:** nobody was moved during the E insert; earlier entries keep their anchors.\n",
    "\n",
    "## Lookup shape (1 hop)\n",
    "\n",
    "- **A, B, C** → compute `prefix(L0)=A` → 1 hop to `(1, A)` → get from tiny container\n",
    "- **D** → `prefix(L0)=9` → hop to `(1, 9)` → get\n",
    "- **E** → `prefix(L0, L1)=A3` → hop to `(2, A3)` → get\n",
    "\n",
    "## Simulated corruption\n",
    "\n",
    "Assume the storage for anchor `(1, A)` is lost/corrupted (e.g., file `anchor/1_A.bin` is gone).\n",
    "\n",
    "### What remains intact\n",
    "```\n",
    "(1, 9) → {D}\n",
    "(2, A3) → {E}\n",
    "```\n",
    "Plus optional metadata you chose to persist (see below).\n",
    "\n",
    "## Self-healing rebuild of the lost anchor\n",
    "\n",
    "### Option A — with per-key insert_depth metadata\n",
    "\n",
    "1. Identify affected keys (e.g., from a lightweight catalog or changelog). For this demo, we know `(1, A)` used to hold `{A, B, C}`\n",
    "\n",
    "2. For each key `k` in that set:\n",
    "   - Read `d = insert_depth(k)` → for A, B, C, `d = 1`\n",
    "   - Recompute `prefix = prefix(k, d)` using `HashChainStream(k)` → all three yield `A`\n",
    "   - Put `k` back into the container for anchor `(1, A)`\n",
    "\n",
    "3. **Result:** `(1, A) → {A, B, C}` exactly as before\n",
    "   - No global scan; no touching `(1, 9)` or `(2, A3)`\n",
    "\n",
    "### Option B — without per-key metadata (replay the policy)\n",
    "\n",
    "1. Start with an empty `(1, A)` container\n",
    "\n",
    "2. **Re-insert A** using the same policy:\n",
    "   - Try `d=1`. `(1, A)` size=0 < `S_MAX` → place\n",
    "\n",
    "3. **Re-insert B:**\n",
    "   - Try `d=1`. size=1 < `S_MAX` → place\n",
    "\n",
    "4. **Re-insert C:**\n",
    "   - Try `d=1`. size=2 < `S_MAX` → place\n",
    "\n",
    "Because the policy only bumps `d` when the target anchor is at/over `S_MAX`, and because the order and key hashes are deterministic, you re-create exactly what existed before the loss: `(1, A)` again holds `{A, B, C}`. You never touched the other anchors.\n",
    "\n",
    "*If original inserts interleaved different anchors, this still works per anchor (you only replay for the keys whose first-d prefix matches that anchor).*\n",
    "\n",
    "## Why this scales\n",
    "\n",
    "- **Locality:** anchors are independent shards. Repair costs are proportional to the size of the damaged anchor, not to the entire table\n",
    "\n",
    "- **Determinism:** the anchor id `(d, prefix)` is a pure function of `(key, policy)`. Replaying policy or reading the stored `insert_depth` lands a key in the same place\n",
    "\n",
    "- **No global rehash:** you never resize or reindex the entire map to recover a small failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c788d7-55ec-463c-8c07-39eae77b8816",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
